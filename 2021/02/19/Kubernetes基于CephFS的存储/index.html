<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="数据卷(Volumes) Kubernetes对于数据卷重新定义,提供了丰富的强大的功能.按照功能可分为三类:本地数据卷,网络数据卷,信息数据卷. Kubernetes提供支持的数据卷类型,最新版本会有增减: 本地数据卷: EmptyDir HostPath   网络数据卷: NFS iSCSI GlusterFS RBD Flocker GCE Perisistent Disk Aws Elas">
<meta name="keywords" content="Kubernetes,Ceph,Docker">
<meta property="og:type" content="article">
<meta property="og:title" content="Kubernetes基于CephFS的存储">
<meta property="og:url" content="http://yoursite.com/2021/02/19/Kubernetes基于CephFS的存储/index.html">
<meta property="og:site_name" content="Sunrise 博客">
<meta property="og:description" content="数据卷(Volumes) Kubernetes对于数据卷重新定义,提供了丰富的强大的功能.按照功能可分为三类:本地数据卷,网络数据卷,信息数据卷. Kubernetes提供支持的数据卷类型,最新版本会有增减: 本地数据卷: EmptyDir HostPath   网络数据卷: NFS iSCSI GlusterFS RBD Flocker GCE Perisistent Disk Aws Elas">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://docs.ceph.org.cn/_images/ditaa-b5a320fc160057a1a7da010b4215489fa66de242.png">
<meta property="og:image" content="http://docs.ceph.org.cn/_images/ditaa-dc9f80d771b55f2daa5cbbfdb2dd0d3e6dfc17c0.png">
<meta property="og:image" content="http://docs.ceph.org.cn/_images/ditaa-50d12451eb76c5c72c4574b08f0320b39a42e5f1.png">
<meta property="og:image" content="http://docs.ceph.org.cn/_images/stack.png">
<meta property="og:image" content="http://docs.ceph.org.cn/_images/ditaa-cffd08dd3e192a5f1d724ad7930cb04200b9b425.png">
<meta property="og:image" content="https://rook.io/docs/rook/v1.0/media/rook-architecture.png">
<meta property="og:image" content="https://rook.io/docs/rook/v1.0/media/kubernetes.png">
<meta property="og:image" content="http://yoursite.com/imgs/mm_reward_qrcode_1525013906055.png">
<meta property="og:updated_time" content="2021-12-14T09:01:00.274Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Kubernetes基于CephFS的存储">
<meta name="twitter:description" content="数据卷(Volumes) Kubernetes对于数据卷重新定义,提供了丰富的强大的功能.按照功能可分为三类:本地数据卷,网络数据卷,信息数据卷. Kubernetes提供支持的数据卷类型,最新版本会有增减: 本地数据卷: EmptyDir HostPath   网络数据卷: NFS iSCSI GlusterFS RBD Flocker GCE Perisistent Disk Aws Elas">
<meta name="twitter:image" content="http://docs.ceph.org.cn/_images/ditaa-b5a320fc160057a1a7da010b4215489fa66de242.png">

<link rel="canonical" href="http://yoursite.com/2021/02/19/Kubernetes基于CephFS的存储/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Kubernetes基于CephFS的存储 | Sunrise 博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Sunrise 博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/02/19/Kubernetes基于CephFS的存储/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://avatars0.githubusercontent.com/u/321919?s=400&u=67298bd9fc3b622eaf93a6cc511a24ac878e86db&v=4">
      <meta itemprop="name" content="yjdwbj">
      <meta itemprop="description" content="最是人间留不住,朱颜辞镜花辞树">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sunrise 博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Kubernetes基于CephFS的存储
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-02-19 14:45:33" itemprop="dateCreated datePublished" datetime="2021-02-19T14:45:33+08:00">2021-02-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-12-14 17:01:00" itemprop="dateModified" datetime="2021-12-14T17:01:00+08:00">2021-12-14</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="数据卷-Volumes"><a href="#数据卷-Volumes" class="headerlink" title="数据卷(Volumes)"></a>数据卷(Volumes)</h1><ul>
<li><code>Kubernetes</code>对于数据卷重新定义,提供了丰富的强大的功能.按照功能可分为三类:本地数据卷,网络数据卷,信息数据卷.</li>
<li><code>Kubernetes</code>提供支持的数据卷类型,最新版本会有增减:<ul>
<li>本地数据卷:<ul>
<li>EmptyDir</li>
<li>HostPath</li>
</ul>
</li>
<li>网络数据卷:<ul>
<li>NFS</li>
<li>iSCSI</li>
<li>GlusterFS</li>
<li>RBD</li>
<li>Flocker</li>
<li>GCE Perisistent Disk</li>
<li>Aws Elastic Block Store</li>
<li>azureDisk</li>
<li>CephFS</li>
<li>fc (fibre Channel)</li>
<li>Persistent Volume Claim</li>
</ul>
</li>
<li>信息数据卷:<ul>
<li>Git Repo(deprecated)</li>
<li>Secret</li>
<li>Downward API</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="本地数据卷"><a href="#本地数据卷" class="headerlink" title="本地数据卷"></a>本地数据卷</h2><h3 id="HostPath"><a href="#HostPath" class="headerlink" title="HostPath"></a><code>HostPath</code></h3><ul>
<li>大多数的<code>Pod</code>应该忽略它们主机节点,因此它们不应该访问节点里文件系统上的任何文件.但是某些系统级别的<code>Pod</code>(通常是由<code>DaemonSet</code>管理)确实需要读取节点里的文件,还有在测试环境中可以<code>HostPath</code>来代替一些<code>PV</code>.<code>HostPath</code>卷指向节点里文件系统是的特定文件或目录.在同一个节点上运行并在其<code>HostPath</code>卷中使用相同路径的<code>Pod</code>可以看到相同的文件.如果要在集群里使用<code>HostPath</code>,需要把<code>--enable-hostpath-provisioner</code>参数标志加到<code>kube-controller-manager</code>里启动.</li>
</ul>
<h1 id="Ceph集群"><a href="#Ceph集群" class="headerlink" title="Ceph集群"></a><code>Ceph</code>集群</h1><ul>
<li><a href="http://docs.ceph.org.cn/" target="_blank" rel="noopener">Ceph 中文官档</a>,<a href="http://docs.ceph.com" target="_blank" rel="noopener">Ceph 英文官档</a>,这里有详尽权威的文档,最新文档及版本,参看<a href="https://github.com/ceph/ceph" target="_blank" rel="noopener">Github Ceph</a>.</li>
</ul>
<h2 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h2><ul>
<li><code>Ceph</code>是一个开源项目,它提供软件定义的(SDS),统一的存储解决方案.具有高度可伸缩性,容量可扩展到<code>EB</code>级别.<code>Ceph</code>的技术特性,总体表现在集群的可靠性,集群扩展性,数据安全性,接口统一性4个方面.</li>
</ul>
<h3 id="数据卷"><a href="#数据卷" class="headerlink" title="数据卷"></a>数据卷</h3><ul>
<li>后端存储可以分为<code>filestore</code>与<code>bluestore</code>:</li>
<li><p>FileStore:</p>
<ul>
<li>FileStore is the legacy approach to storing objects in Ceph. It relies on a standard file system (normally XFS) in combination with a key/value database (traditionally LevelDB, now RocksDB) for some metadata.</li>
<li>FileStore is well-tested and widely used in production. However, it suffers from many performance deficiencies due to its overall design and its reliance on a traditional file system for object data storage.</li>
<li>Although FileStore is capable of functioning on most POSIX-compatible file systems (including btrfs and ext4), we recommend that only the XFS file system be used with Ceph. Both btrfs and ext4 have known bugs and deficiencies and their use may lead to data loss. By default, all Ceph provisioning tools use XFS.</li>
</ul>
</li>
<li><p>BlueStore:</p>
<ul>
<li>Key BlueStore features include:<ul>
<li>Direct management of storage devices. BlueStore consumes raw block devices or partitions. This avoids intervening layers of abstraction (such as local file systems like XFS) that can limit performance or add complexity.</li>
<li>Metadata management with RocksDB. RocksDB’s key/value database is embedded in order to manage internal metadata, including the mapping of object names to block locations on disk.</li>
<li>Full data and metadata checksumming. By default, all data and metadata written to BlueStore is protected by one or more checksums. No data or metadata is read from disk or returned to the user without being verified.</li>
<li>Inline compression. Data can be optionally compressed before being written to disk.</li>
<li>Multi-device metadata tiering. BlueStore allows its internal journal (write-ahead log) to be written to a separate, high-speed device (like an SSD, NVMe, or NVDIMM) for increased performance. If a significant amount of faster storage is available, internal metadata can be stored on the faster device.</li>
<li>Efficient copy-on-write. RBD and CephFS snapshots rely on a copy-on-write clone mechanism that is implemented efficiently in BlueStore. This results in efficient I/O both for regular snapshots and for erasure-coded pools (which rely on cloning to implement efficient two-phase commits).</li>
</ul>
</li>
<li>支持下面的配置:<ul>
<li>A block device, a block.wal, and a block.db device</li>
<li>A block device and a block.wal device</li>
<li>A block device and a block.db device</li>
<li>A single block device</li>
</ul>
</li>
<li>block device 也有三种选项:<ul>
<li>整个磁盘</li>
<li>磁盘分区</li>
<li>逻辑卷 (a logical volumen of LVM)</li>
</ul>
</li>
</ul>
</li>
<li><p>注意:</p>
<ol>
<li>不可以使用磁盘作为<code>block.db</code>或者<code>block.wal</code>,否则会报错:<code>blkid could not detect a PARTUUID for device</code>;</li>
<li>若使用磁盘或者分区作<code>block</code>,则<code>ceph-volume</code>会在其上创建<code>LV</code>来使用.若使用分区作<code>block.db</code>或<code>block.wal</code>,则直接使用分区而不创建<code>LV</code>.</li>
</ol>
</li>
<li><p><code>BlueFs</code>将整个<code>BlueStore</code>的存储空间分为三个层次:</p>
<ul>
<li>慢速(Slow)空间:主要用于存储对象数据,可由普通大容量机械盘提供,由<code>BlueStore</code>自行管理</li>
<li>高速(DB)空间:存储<code>BlueStore</code>内部产生的元数据，可由普通<code>SSD</code>提供，需求小于(慢速空间).</li>
<li>超高速(WAL)空间:主要存储<code>RocksDB</code>内部产生的.log 文件,可由<code>SSD</code>或者<code>NVRAM</code>等时延相较普通<code>SSD</code>更小的设备充当.容量需求和(高速空间)相当，同样由<code>Bluefs</code>直接管理.</li>
</ul>
</li>
</ul>
<h2 id="Ceph的功能组件"><a href="#Ceph的功能组件" class="headerlink" title="Ceph的功能组件"></a><code>Ceph</code>的功能组件</h2><ul>
<li><code>Ceph OSD</code>:(Object Storage Device),主要功能包括存储数据,处理数据的复制,恢复,回补平衡数据分布,并将一些相关数据提供给<code>Ceph Monitor</code>,例如 <code>Ceph OSD</code>心跳等.一个<code>Ceph</code>的存储集群,至少需要1个<code>Ceph OSD</code>来实现<code>active+clean</code>健康状态和有效的保存数据的副本(默认情况下是双副本,可以调整).注意:每一个<code>Disk</code>,分区都可以成为一个<code>OSD</code>.</li>
<li><code>Ceph Monitor</code>:<code>Ceph</code>的监视器,主要功能是维护整个集群健康状态,提供一致性的决策,包含了 <code>Monitor map</code>,<code>OSD map</code>,<code>PG(Placement Group) map</code>和<code>CRUSH map</code>.</li>
<li><code>Ceph MDS</code>:(Ceph Metadata Server),主要保存的是<code>Ceph</code>文件系统的元数据(metadata).注意:<code>Ceph</code>的块存储与<code>Ceph</code>的对象存储都不需要<code>Ceph MDS</code>.<code>Ceph MDS</code>为基于<code>POSIX</code>文件系统的用户提供了一些基础命令,如:<code>ls,find</code>等.如果需要创建<code>CephFS</code>才需要用到<code>MDS</code>,但是<code>CephFS</code>离生产使用还有一段距离.</li>
</ul>
<h2 id="Ceph-功能特性"><a href="#Ceph-功能特性" class="headerlink" title="Ceph 功能特性"></a>Ceph 功能特性</h2><h3 id="RADOS"><a href="#RADOS" class="headerlink" title="RADOS"></a>RADOS</h3><ul>
<li><code>RADOS</code>具备自我修复等特性,提供了一个可靠,自动,智能的分布式存储.它的灵魂<code>CRUSH(Controlled Replication Under Scalable Hashing,可扩展哈希算法的可控复制)</code>算法.</li>
</ul>
<h3 id="Ceph文件系统"><a href="#Ceph文件系统" class="headerlink" title="Ceph文件系统"></a><code>Ceph</code>文件系统</h3><ul>
<li><code>CephFS</code>功能特性是基于<code>RADOS</code>来现实分布式的文件系统,引入了<code>MDS(Metadata Server)</code>,主要为兼容<code>POSIX</code>文件系统提供元数据.一般都是当体系文件系统来挂载.</li>
<li><code>Ceph</code>文件系统<br><img src="http://docs.ceph.org.cn/_images/ditaa-b5a320fc160057a1a7da010b4215489fa66de242.png" alt="ditaa-b5a320fc160057a1a7da010b4215489fa66de242"></li>
</ul>
<h3 id="Ceph块设备"><a href="#Ceph块设备" class="headerlink" title="Ceph块设备"></a><code>Ceph</code>块设备</h3><ul>
<li><p><code>RBD(Rados Block Device)</code>功能特性是基于<code>Librados</code>之上,通过<code>Librbd</code>创建一个块设备,通过<code>QEMU/KVM</code>附加到<code>VM</code>上,作为传统的块设备来使用.目前<code>OpenStack,CloudStack</code>等都是采用这种方式来为<code>VM</code>提供块设备,同时也支持快照同<code>COW(Copy On Write)</code>等功能.</p>
</li>
<li><p><code>Ceph</code>块设备<br><img src="http://docs.ceph.org.cn/_images/ditaa-dc9f80d771b55f2daa5cbbfdb2dd0d3e6dfc17c0.png" alt="ditaa-dc9f80d771b55f2daa5cbbfdb2dd0d3e6dfc17c0"></p>
</li>
</ul>
<h3 id="Ceph对象网关"><a href="#Ceph对象网关" class="headerlink" title="Ceph对象网关"></a><code>Ceph</code>对象网关</h3><ul>
<li><p><code>RADWOGW</code>的功能特性是基于<code>LibRADOS</code>之上,提供当前流行的<code>RESTful</code>协议的网关,并且兼容<code>AWS S3</code>和<code>Swift</code>接口,作为对象存储,可以对接网盘类应用以及<code>HLS</code>的流媒体应用等.<br><img src="http://docs.ceph.org.cn/_images/ditaa-50d12451eb76c5c72c4574b08f0320b39a42e5f1.png" alt="ditaa-50d12451eb76c5c72c4574b08f0320b39a42e5f1"></p>
</li>
<li><p>体系结构<br><img src="http://docs.ceph.org.cn/_images/stack.png" alt="stack"></p>
</li>
</ul>
<h2 id="通过Ceph-ceph-ansiable安装"><a href="#通过Ceph-ceph-ansiable安装" class="headerlink" title="通过Ceph/ceph-ansiable安装"></a>通过<code>Ceph/ceph-ansiable</code>安装</h2><ul>
<li><a href="https://github.com/ceph/ceph-ansible" target="_blank" rel="noopener">ceph-ansible</a></li>
</ul>
<ul>
<li>关于<code>Releases</code>版本的特别说明:<ul>
<li><code>stable-3.0</code>支持<code>ceph jewel 和 luminous 版本.该</code>branch<code>需要</code>Ansible 2.4`版本.</li>
<li><code>stable-3.1</code>支持<code>ceph luminous 和 mimic 版本.该</code>branch<code>需要</code>Ansible 2.4`版本.</li>
<li><code>stable-3.2</code>支持<code>ceph luminous 和 mimic 版本.该</code>branch<code>需要</code>Ansible 2.6`版本.</li>
<li><code>stable-4.0</code>支持<code>ceph nautilus 版本.该</code>branch<code>需要</code>Ansible 2.8`版本.</li>
<li><code>master</code>支持<code>Ceph@master</code>版本.该<code>branch</code>需要<code>Ansible 2.8</code>版本.</li>
</ul>
</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">~$ git <span class="built_in">clone</span>  https://github.com/ceph/ceph-ansible</span><br><span class="line">~$ git checkout -b v3.2.9</span><br><span class="line">~$ pip install -r  ceph-ansible/requirements.txt</span><br></pre></td></tr></table></figure>
<h2 id="通过Ceph-ceph-deploy安装"><a href="#通过Ceph-ceph-deploy安装" class="headerlink" title="通过Ceph/ceph-deploy安装"></a>通过<code>Ceph/ceph-deploy</code>安装</h2><ul>
<li><p>系统环境:</p>
<ul>
<li>ceph: version 10.2.11 (e4b061b47f07f583c92a050d9e84b1813a35671e)</li>
<li>Debian GNU/Linux 9.9 (stretch)</li>
<li>ceph-deploy : 2.0.1</li>
</ul>
</li>
<li><p>链接:</p>
<ul>
<li><a href="https://github.com/ceph/ceph-ansible" target="_blank" rel="noopener">ceph-ansible</a></li>
<li><a href="https://github.com/ceph/ceph-deploy" target="_blank" rel="noopener">Deploy Ceph with minimal infrastructure, using just SSH access</a></li>
<li><a href="https://blog.csdn.net/for_tech/article/details/80271495" target="_blank" rel="noopener">refer1</a></li>
<li><a href="https://stackoverflow.com/questions/39979605/using-ceph-deploy-when-apt-repos-are-down" target="_blank" rel="noopener">Using ceph-deploy when APT repos are down</a></li>
<li><a href="http://www.yangguanjun.com/2018/04/06/ceph-deploy-latest-luminous/" target="_blank" rel="noopener">refer2</a></li>
<li><a href="http://docs.ceph.com/ceph-deploy/docs/repo.html" target="_blank" rel="noopener">refer3</a></li>
</ul>
</li>
</ul>
<h3 id="快速安装-apt"><a href="#快速安装-apt" class="headerlink" title="快速安装(apt)"></a>快速安装(apt)</h3><ul>
<li><code>Ceph-deploy</code>是比较旧的部署方式,过程稍复杂.经测试使用<code>apt</code>的方式安装不到<code>ceph-deploy</code>,这里通过<code>pip install ceph-deploy</code>安装成功.</li>
<li>下面使用<code>VirtualBox</code>创建虚拟机来做实验.创建一个<code>Linux</code>虚拟机,安装<a href="https://saimei.ftp.acc.umu.se/debian-cd/current/amd64/iso-cd/debian-9.9.0-amd64-netinst.iso" target="_blank" rel="noopener">debian9</a>,设置两个网卡,一个是<code>NAT(10.0.2.0/24)</code>用于外网下载软件使用,一个是<code>Vboxnet1(192.168.99.0/24)</code>集群通信使用.安装一些常的工具软件,并克隆4个新的虚拟机,更改它的主机名与IP地址.下面会用到<code>Ansible</code>来批量操作这些虚拟机.安装虚假机结构如下:</li>
</ul>
<p><img src="http://docs.ceph.org.cn/_images/ditaa-cffd08dd3e192a5f1d724ad7930cb04200b9b425.png" alt="cffd08dd3e192a5f1d724ad7930cb04200b9b425"></p>
<ul>
<li>在上述的节点虚拟器机里安装<code>apt get ntp ntpdate ntp-doc</code>时间服务器相关包,并配置好<code>SSH</code>公钥免农密登录.这里可以使用<code>Ansible</code>变量操作.</li>
<li><strong>注意</strong>,各个节点里的<code>/etc/hosts</code>要与<code>ceph-deploy</code>操作的主机是一致的.否则会出现<code>ceph-deploy mon create-initial</code>无法进行的错误.</li>
</ul>
<h3 id="清理旧节点"><a href="#清理旧节点" class="headerlink" title="清理旧节点"></a>清理旧节点</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">~$ ceph-deploy purge &#123;ceph-node&#125; [&#123;ceph-node&#125;]</span><br><span class="line">~$ ceph-deploy purgedata &#123;ceph-node&#125; [&#123;ceph-node&#125;]</span><br><span class="line">~$ ceph-deploy forgetkeys</span><br><span class="line">~$ rm ceph.*</span><br></pre></td></tr></table></figure>
<h3 id="安装节点"><a href="#安装节点" class="headerlink" title="安装节点"></a>安装节点</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">~$ ceph-deploy new --<span class="built_in">help</span></span><br><span class="line">usage: ceph-deploy new [-h] [--no-ssh-copykey] [--fsid FSID]</span><br><span class="line">                       [--cluster-network CLUSTER_NETWORK]</span><br><span class="line">                       [--public-network PUBLIC_NETWORK]</span><br><span class="line">                       MON [MON ...]</span><br><span class="line"></span><br><span class="line">Start deploying a new cluster, and write a CLUSTER.conf and keyring <span class="keyword">for</span> it.</span><br><span class="line"></span><br><span class="line">positional arguments:</span><br><span class="line">  MON                   initial monitor hostname, fqdn, or hostname:fqdn pair</span><br><span class="line"></span><br><span class="line">optional arguments:</span><br><span class="line">  -h, --<span class="built_in">help</span>            show this <span class="built_in">help</span> message and <span class="built_in">exit</span></span><br><span class="line">  --no-ssh-copykey      <span class="keyword">do</span> not attempt to copy SSH keys</span><br><span class="line">  --fsid FSID           provide an alternate FSID <span class="keyword">for</span> ceph.conf generation</span><br><span class="line">  --cluster-network CLUSTER_NETWORK</span><br><span class="line">                        specify the (internal) cluster network</span><br><span class="line">  --public-network PUBLIC_NETWORK</span><br><span class="line">                        specify the public network <span class="keyword">for</span> a cluster</span><br><span class="line"></span><br><span class="line">~$ ceph-deploy new node1 node2 node3 --public-network 192.168.99.0/24</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面命令,脚本会通过ssh到每个节点上安装相应的ceph包.类似于 apt install ceph ceph-base ceph-common ceph-mds ceph-mon ceph-osd radosgw</span></span><br><span class="line"><span class="comment"># 因为使用的是cepy-deplopy 2.0.x,必须要指定为luminous(v12)以上版本,否则它是默认安装Mimic(v10)的版本.现在最新版是 Nautilus(v14.0.2)</span></span><br><span class="line">~$ ceph-deploy install --release luminous node1 node2 node3</span><br><span class="line"><span class="comment"># 可以加入这两个参数，加速安装 --repo-url http://mirrors.ustc.edu.cn/ceph/debian-luminous --gpg-url http://mirrors.ustc.edu.cn/ceph/keys/release.asc</span></span><br><span class="line"><span class="comment"># 上述命令后,会在当前目录下创建ceph.conf,ceph-mon.keyring</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>Pool,PG(Placement Groups)</code>和<code>CRUSH</code>配置参考,<a href="http://docs.ceph.com/docs/master/rados/configuration/pool-pg-config-ref/" target="_blank" rel="noopener">官方文档</a>,下面<code>PG</code>参数的调校可以参照这里<a href="https://ceph.com/pgcalc/" target="_blank" rel="noopener">PgCalc</a></li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --&gt; ceph.conf</span></span><br><span class="line">[global]</span><br><span class="line"></span><br><span class="line">	<span class="comment"># By default, Ceph makes 3 replicas of objects. If you want to make four</span></span><br><span class="line">	<span class="comment"># copies of an object the default value--a primary copy and three replica</span></span><br><span class="line">	<span class="comment"># copies--reset the default values as shown in 'osd pool default size'.</span></span><br><span class="line">	<span class="comment"># If you want to allow Ceph to write a lesser number of copies in a degraded</span></span><br><span class="line">	<span class="comment"># state, set 'osd pool default min size' to a number less than the</span></span><br><span class="line">	<span class="comment"># 'osd pool default size' value.</span></span><br><span class="line"></span><br><span class="line">	osd pool default size = 3  <span class="comment"># Write an object 3 times.</span></span><br><span class="line">	osd pool default min size = 2 <span class="comment"># Allow writing two copies in a degraded state.</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># Ensure you have a realistic number of placement groups. We recommend</span></span><br><span class="line">	<span class="comment"># approximately 100 per OSD. E.g., total number of OSDs multiplied by 100</span></span><br><span class="line">	<span class="comment"># divided by the number of replicas (i.e., osd pool default size). So for</span></span><br><span class="line">	<span class="comment"># 10 OSDs and osd pool default size = 4, we'd recommend approximately</span></span><br><span class="line">	<span class="comment"># (100 * 10) / 4 = 250.</span></span><br><span class="line"></span><br><span class="line">	osd pool default pg num = 250</span><br><span class="line">	osd pool default pgp num = 250</span><br></pre></td></tr></table></figure>
<ul>
<li><p>初始化<code>Monitors</code></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">~$ ceph-deploy mon create node1 node2 node3</span><br><span class="line">~$ ceph-deploy gatherkeys node1 node2 node3</span><br></pre></td></tr></table></figure>
</li>
<li><p>注意:如果出现下面的错误，可能是系统的空间小于<code>5%</code>.具体错误细节可以查看<code>/var/log/ceph/ceph-mon.DB001.log</code></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[DB001][INFO  ] Running <span class="built_in">command</span>: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.DB001.asok mon_status</span><br><span class="line">[DB001][ERROR ] b<span class="string">'admin_socket: exception getting command descriptions: [Errno 2] No such file or directory'</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>会在当前目录下,创建如下文件</p>
<ul>
<li>{cluster-name}.client.admin.keyring</li>
<li>{cluster-name}.mon.keyring</li>
<li>{cluster-name}.bootstrap-osd.keyring</li>
<li>{cluster-name}.bootstrap-mds.keyring</li>
<li>{cluster-name}.bootstrap-rgw.keyring</li>
<li>{cluster-name}.bootstrap-mgr.keyring</li>
</ul>
</li>
<li><p>分发 ceph 配置和 keys 到集群的节点中去:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">$ ceph-deploy admin node1 node2 node3</span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /home/lcy/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /home/lcy/.pyenv/versions/py3dev/bin/ceph-deploy admin node1 node2 node3</span><br><span class="line">[ceph_deploy.cli][INFO  ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO  ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO  ]  client                        : [<span class="string">'node1'</span>, <span class="string">'node2'</span>, <span class="string">'node3'</span>]</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf object at 0x7fef98f77390&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  default_release               : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  func                          : &lt;<span class="keyword">function</span> admin at 0x7fef99bdd6a8&gt;</span><br><span class="line">[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to node1</span><br><span class="line">[node1][DEBUG ] connection detected need <span class="keyword">for</span> sudo</span><br><span class="line">[node1][DEBUG ] connected to host: node1</span><br><span class="line">[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to node2</span><br><span class="line">[node2][DEBUG ] connection detected need <span class="keyword">for</span> sudo</span><br><span class="line">[node2][DEBUG ] connected to host: node2</span><br><span class="line">[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to node3</span><br><span class="line">[node3][DEBUG ] connection detected need <span class="keyword">for</span> sudo</span><br><span class="line">[node3][DEBUG ] connected to host: node3</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看集群的状态,可直接登录用<code>root</code>权限运行,或者<code>Ansible</code>命令运行:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">~$ ansible -i ../hosts node1 -b -m <span class="built_in">command</span> -a <span class="string">"ceph -s"</span></span><br><span class="line">192.168.99.101 | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">  cluster:</span><br><span class="line">    id:     0bf150da-b691-4382-bf3d-600e90c19fba</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum node1,node2,node3</span><br><span class="line">    mgr: no daemons active</span><br><span class="line">    osd: 0 osds: 0 up, 0 <span class="keyword">in</span></span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0B</span><br><span class="line">    usage:   0B used, 0B / 0B avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Ceph-Manager部署"><a href="#Ceph-Manager部署" class="headerlink" title="Ceph Manager部署"></a><code>Ceph Manager</code>部署</h3><ul>
<li><a href="http://docs.ceph.com/docs/master/mgr/" target="_blank" rel="noopener">参考文档</a><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">~$ ceph-deploy mgr create node1 node2 node3</span><br><span class="line">[...]</span><br><span class="line">~$ ansible -i ../hosts node1 -b -m <span class="built_in">command</span> -a <span class="string">"ceph -s"</span></span><br><span class="line">192.168.99.101 | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">  cluster:</span><br><span class="line">    id:     0bf150da-b691-4382-bf3d-600e90c19fba</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum node1,node2,node3</span><br><span class="line">    mgr: node1(active), standbys: node3, node2</span><br><span class="line">    osd: 0 osds: 0 up, 0 <span class="keyword">in</span></span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0B</span><br><span class="line">    usage:   0B used, 0B / 0B avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Ceph-OSD部署"><a href="#Ceph-OSD部署" class="headerlink" title="Ceph OSD部署"></a><code>Ceph OSD</code>部署</h3><ul>
<li><a href="http://docs.ceph.com/docs/master/ceph-volume/" target="_blank" rel="noopener">ceph-volume</a></li>
<li>从<code>Ceph Luminous 12.2.2</code>开始,<code>ceph-disk</code>被弃用,使用<code>ceph-volume</code>代替.</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">~$ ceph-deploy osd -h</span><br><span class="line">usage: ceph-deploy osd [-h] &#123;list,create&#125; ...</span><br><span class="line"></span><br><span class="line">Create OSDs from a data disk on a remote host:</span><br><span class="line"></span><br><span class="line">    ceph-deploy osd create &#123;node&#125; --data /path/to/device</span><br><span class="line"></span><br><span class="line">For bluestore, optional devices can be used::</span><br><span class="line"></span><br><span class="line">    ceph-deploy osd create &#123;node&#125; --data /path/to/data --block-db /path/to/db-device</span><br><span class="line">    ceph-deploy osd create &#123;node&#125; --data /path/to/data --block-wal /path/to/wal-device</span><br><span class="line">    ceph-deploy osd create &#123;node&#125; --data /path/to/data --block-db /path/to/db-device --block-wal /path/to/wal-device</span><br><span class="line"></span><br><span class="line">For filestore, the journal must be specified, as well as the objectstore::</span><br><span class="line"></span><br><span class="line">    ceph-deploy osd create &#123;node&#125; --filestore --data /path/to/data --journal /path/to/journal</span><br><span class="line"></span><br><span class="line">For data devices, it can be an existing logical volume <span class="keyword">in</span> the format of:</span><br><span class="line">vg/lv, or a device. For other OSD components like wal, db, and journal, it</span><br><span class="line">can be logical volume (<span class="keyword">in</span> vg/lv format) or it must be a GPT partition.</span><br><span class="line"></span><br><span class="line">positional arguments:</span><br><span class="line">  &#123;list,create&#125;</span><br><span class="line">    list         List OSD info from remote host(s)</span><br><span class="line">    create       Create new Ceph OSD daemon by preparing and activating a</span><br><span class="line">                 device</span><br><span class="line"></span><br><span class="line">optional arguments:</span><br><span class="line">  -h, --<span class="built_in">help</span>     show this <span class="built_in">help</span> message and <span class="built_in">exit</span></span><br><span class="line"></span><br><span class="line">~$ ceph-deploy osd create -h</span><br><span class="line">usage: ceph-deploy osd create [-h] [--data DATA] [--journal JOURNAL]</span><br><span class="line">                              [--zap-disk] [--fs-type FS_TYPE] [--dmcrypt]</span><br><span class="line">                              [--dmcrypt-key-dir KEYDIR] [--filestore]</span><br><span class="line">                              [--bluestore] [--block-db BLOCK_DB]</span><br><span class="line">                              [--block-wal BLOCK_WAL] [--debug]</span><br><span class="line">                              [HOST]</span><br><span class="line"></span><br><span class="line">positional arguments:</span><br><span class="line">  HOST                  Remote host to connect</span><br><span class="line"></span><br><span class="line">optional arguments:</span><br><span class="line">  -h, --<span class="built_in">help</span>            show this <span class="built_in">help</span> message and <span class="built_in">exit</span></span><br><span class="line">  --data DATA           The OSD data logical volume (vg/lv) or absolute path</span><br><span class="line">                        to device</span><br><span class="line">  --journal JOURNAL     Logical Volume (vg/lv) or path to GPT partition</span><br><span class="line">  --zap-disk            DEPRECATED - cannot zap when creating an OSD</span><br><span class="line">  --fs-type FS_TYPE     filesystem to use to format DEVICE (xfs, btrfs)</span><br><span class="line">  --dmcrypt             use dm-crypt on DEVICE</span><br><span class="line">  --dmcrypt-key-dir KEYDIR</span><br><span class="line">                        directory <span class="built_in">where</span> dm-crypt keys are stored</span><br><span class="line">  --filestore           filestore objectstore</span><br><span class="line">  --bluestore           bluestore objectstore</span><br><span class="line">  --block-db BLOCK_DB   bluestore block.db path</span><br><span class="line">  --block-wal BLOCK_WAL</span><br><span class="line">                        bluestore block.wal path</span><br><span class="line">  --debug               Enable debug mode on remote ceph-volume calls</span><br></pre></td></tr></table></figure>
<ul>
<li>在<code>node1</code>添加了一个磁盘作<code>OSD</code>盘,下面把它整个盘创建成一个块设备.</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">~$ ceph-deploy osd create node1 --data /dev/vdb</span><br><span class="line">~$ ceph-deploy osd create node2 --data /dev/vdb</span><br><span class="line">~$ ceph-deploy osd create node3 --data /dev/vdb</span><br></pre></td></tr></table></figure>
<ul>
<li>出错消息，如果原盘里面有<code>LVM</code>的信息，要先手动清除原<code>LVM</code>信息，不然会出现下面错误.先用<code>lvdisplay</code>查看，再用<code>lvremove --force</code>,<code>vgdisplay</code>,<code>vgremove --force</code>,清除原有的 LVM 信息.</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[DB001][ERROR ] RuntimeError: <span class="built_in">command</span> returned non-zero <span class="built_in">exit</span> status: 1</span><br><span class="line">[ceph_deploy.osd][ERROR ] Failed to execute <span class="built_in">command</span>: /usr/sbin/ceph-volume --cluster ceph lvm create --bluestore --data /dev/vdb</span><br><span class="line">[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs</span><br></pre></td></tr></table></figure>
<ul>
<li><p>清除原<code>LVM</code>信息</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~$ ansible -i hosts all  -b -m shell  -a <span class="string">"lvdisplay | awk 'NR==2 &#123;print <span class="variable">$3</span>&#125;'| xargs  lvremove --force ;  vgdisplay | awk 'NR==2 &#123;print <span class="variable">$3</span>&#125;' | xargs  vgremove"</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>查看<code>osd</code>状态</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">~$ ansible -i ../hosts node1 -b -m <span class="built_in">command</span> -a <span class="string">"ceph osd stat"</span></span><br><span class="line">3 osds: 3 up, 3 <span class="keyword">in</span></span><br><span class="line"></span><br><span class="line">~$ ansible -i ../hosts node1 -b -m <span class="built_in">command</span> -a <span class="string">"ceph  df"</span></span><br><span class="line">GLOBAL:</span><br><span class="line">    SIZE       AVAIL      RAW USED     %RAW USED</span><br><span class="line">    180GiB     177GiB      3.02GiB          1.68</span><br><span class="line">POOLS:</span><br><span class="line">    NAME                ID     USED        %USED     MAX AVAIL     OBJECTS</span><br><span class="line">    hdd                 1         375B         0       84.0GiB           8</span><br><span class="line">    cephfs_data         2           0B         0       84.0GiB           0</span><br><span class="line">    cephfs_metadata     3      2.19KiB         0       84.0GiB          21</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看<code>osd</code>树</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ ansible -i hosts node1  -b -m <span class="built_in">command</span> -a <span class="string">"ceph osd tree"</span></span><br><span class="line"></span><br><span class="line">node1 | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF</span><br><span class="line">-1       0.17578 root default</span><br><span class="line">-3       0.05859     host node1</span><br><span class="line"> 0   hdd 0.05859         osd.0       up  1.00000 1.00000</span><br><span class="line">-7       0.05859     host node2</span><br><span class="line"> 2   hdd 0.05859         osd.2       up  1.00000 1.00000</span><br><span class="line">-5       0.05859     host node3</span><br><span class="line"> 1   hdd 0.05859         osd.1       up  1.00000 1.00000</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看系统状态</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">$ ansible -i ../hosts node1 -b -m <span class="built_in">command</span> -a <span class="string">"ceph -s"</span></span><br><span class="line">192.168.99.101 | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">  cluster:</span><br><span class="line">    id:     0bf150da-b691-4382-bf3d-600e90c19fba</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum node1,node2,node3</span><br><span class="line">    mgr: node1(active), standbys: node3, node2</span><br><span class="line">    osd: 1 osds: 1 up, 1 <span class="keyword">in</span></span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0B</span><br><span class="line">    usage:   1.00GiB used, 9.00GiB / 10.0GiB avail  <span class="comment"># 这里是node1:/dev/sdb刚创建的.</span></span><br><span class="line">    pgs:</span><br><span class="line"><span class="comment"># 如果要使用管道操作,必须使用shell模块,command模块会出错.</span></span><br><span class="line">~$ ansible -i ../hosts node1 -b -m shell -a <span class="string">"mount | grep ceph"</span></span><br><span class="line">tmpfs on /var/lib/ceph/osd/ceph-0 <span class="built_in">type</span> tmpfs (rw,relatime)</span><br><span class="line"></span><br><span class="line">~$ ansible -i ../hosts node1 -b -m <span class="built_in">command</span> -a <span class="string">"ls -l /var/lib/ceph/"</span></span><br><span class="line">192.168.99.101 | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">total 44</span><br><span class="line">drwxr-xr-x 2 ceph ceph 4096 Apr 11 08:44 bootstrap-mds</span><br><span class="line">drwxr-xr-x 2 ceph ceph 4096 May  9 21:33 bootstrap-mgr</span><br><span class="line">drwxr-xr-x 2 ceph ceph 4096 May  9 22:27 bootstrap-osd</span><br><span class="line">drwxr-xr-x 2 ceph ceph 4096 Apr 11 08:44 bootstrap-rbd</span><br><span class="line">drwxr-xr-x 2 ceph ceph 4096 Apr 11 08:44 bootstrap-rgw</span><br><span class="line">drwxr-xr-x 2 ceph ceph 4096 Apr 11 08:44 mds</span><br><span class="line">drwxr-xr-x 3 ceph ceph 4096 May  9 21:33 mgr</span><br><span class="line">drwxr-xr-x 3 ceph ceph 4096 May  9 21:22 mon</span><br><span class="line">drwxr-xr-x 3 ceph ceph 4096 May  9 22:27 osd</span><br><span class="line">drwxr-xr-x 2 ceph ceph 4096 Apr 11 08:44 radosgw</span><br><span class="line">drwxr-xr-x 2 ceph ceph 4096 May  9 21:22 tmp</span><br><span class="line"></span><br><span class="line">~$ ansible -i ../hosts node1 -b -m <span class="built_in">command</span> -a <span class="string">"ls -l /var/lib/ceph/osd/ceph-0"</span></span><br><span class="line">192.168.99.101 | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">total 48</span><br><span class="line">-rw-r--r-- 1 ceph ceph 393 May  9 22:27 activate.monmap</span><br><span class="line">lrwxrwxrwx 1 ceph ceph  93 May  9 22:27 block -&gt; /dev/ceph-195012d6-0c8a-45bf-964c-3ac15f2cd024/osd-block-261c9455-fbc4-4eba-9783-5fba4290048d</span><br><span class="line">-rw-r--r-- 1 ceph ceph   2 May  9 22:27 bluefs</span><br><span class="line">-rw-r--r-- 1 ceph ceph  37 May  9 22:27 ceph_fsid</span><br><span class="line">-rw-r--r-- 1 ceph ceph  37 May  9 22:27 fsid</span><br><span class="line">-rw------- 1 ceph ceph  55 May  9 22:27 keyring</span><br><span class="line">-rw-r--r-- 1 ceph ceph   8 May  9 22:27 kv_backend</span><br><span class="line">-rw-r--r-- 1 ceph ceph  21 May  9 22:27 magic</span><br><span class="line">-rw-r--r-- 1 ceph ceph   4 May  9 22:27 mkfs_done</span><br><span class="line">-rw-r--r-- 1 ceph ceph  41 May  9 22:27 osd_key</span><br><span class="line">-rw-r--r-- 1 ceph ceph   6 May  9 22:27 ready</span><br><span class="line">-rw-r--r-- 1 ceph ceph  10 May  9 22:27 <span class="built_in">type</span></span><br><span class="line">-rw-r--r-- 1 ceph ceph   2 May  9 22:27 whoami</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看<code>Ceph</code>的参数配置项</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">~$ ansible -i ../hosts node1 -b -m <span class="built_in">command</span> -a <span class="string">"ceph --show-config"</span></span><br><span class="line">name = client.admin</span><br><span class="line">cluster = ceph</span><br><span class="line">debug_none = 0/5</span><br><span class="line">debug_lockdep = 0/1</span><br><span class="line">[....]</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看<code>LVM</code>相关信息</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">~$ ansible -i ../hosts node1 -b -m <span class="built_in">command</span> -a <span class="string">"pvdisplay"</span></span><br><span class="line">192.168.99.101 | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">  --- Physical volume ---</span><br><span class="line">  PV Name               /dev/sdb</span><br><span class="line">  VG Name               ceph-195012d6-0c8a-45bf-964c-3ac15f2cd024</span><br><span class="line">  PV Size               10.00 GiB / not usable 4.00 MiB</span><br><span class="line">  Allocatable           yes (but full)</span><br><span class="line">  PE Size               4.00 MiB</span><br><span class="line">  Total PE              2559</span><br><span class="line">  Free PE               0</span><br><span class="line">  Allocated PE          2559</span><br><span class="line">  PV UUID               Qd6kSs-Ivbp-3APy-21Tv-XQgx-EhBn-XfioVa</span><br><span class="line"></span><br><span class="line">~$ ansible -i ../hosts node1 -b -m <span class="built_in">command</span> -a <span class="string">"vgdisplay"</span></span><br><span class="line">192.168.99.101 | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">  --- Volume group ---</span><br><span class="line">  VG Name               ceph-195012d6-0c8a-45bf-964c-3ac15f2cd024</span><br><span class="line">  System ID</span><br><span class="line">  Format                lvm2</span><br><span class="line">  Metadata Areas        1</span><br><span class="line">  Metadata Sequence No  17</span><br><span class="line">  VG Access             <span class="built_in">read</span>/write</span><br><span class="line">  VG Status             resizable</span><br><span class="line">  MAX LV                0</span><br><span class="line">  Cur LV                1</span><br><span class="line">  Open LV               1</span><br><span class="line">  Max PV                0</span><br><span class="line">  Cur PV                1</span><br><span class="line">  Act PV                1</span><br><span class="line">  VG Size               10.00 GiB</span><br><span class="line">  PE Size               4.00 MiB</span><br><span class="line">  Total PE              2559</span><br><span class="line">  Alloc PE / Size       2559 / 10.00 GiB</span><br><span class="line">  Free  PE / Size       0 / 0</span><br><span class="line">  VG UUID               XiVkQ6-aUPv-3BRw-Gj1N-jdG4-HRxf-hCS3Mg</span><br><span class="line"></span><br><span class="line">~$ ansible -i ../hosts node1 -b -m <span class="built_in">command</span> -a <span class="string">"lvdisplay"</span></span><br><span class="line">192.168.99.101 | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/ceph-195012d6-0c8a-45bf-964c-3ac15f2cd024/osd-block-261c9455-fbc4-4eba-9783-5fba4290048d</span><br><span class="line">  LV Name                osd-block-261c9455-fbc4-4eba-9783-5fba4290048d</span><br><span class="line">  VG Name                ceph-195012d6-0c8a-45bf-964c-3ac15f2cd024</span><br><span class="line">  LV UUID                F9dF0S-qwb7-LJC0-vld2-TF6g-nP8q-9ncsdI</span><br><span class="line">  LV Write Access        <span class="built_in">read</span>/write</span><br><span class="line">  LV Creation host, time node1, 2019-05-09 22:27:44 -0400</span><br><span class="line">  LV Status              available</span><br><span class="line">  <span class="comment"># open                 4</span></span><br><span class="line">  LV Size                10.00 GiB</span><br><span class="line">  Current LE             2559</span><br><span class="line">  Segments               1</span><br><span class="line">  Allocation             inherit</span><br><span class="line">  Read ahead sectors     auto</span><br><span class="line">  - currently <span class="built_in">set</span> to     256</span><br><span class="line">  Block device           253:0</span><br></pre></td></tr></table></figure>
</li>
<li><p>关于<code>LVM</code>的关系.简单描述一下:<code>LV</code>是建立在<code>VG</code>上,<code>VG</code>建立在<code>PV</code>上面.</p>
</li>
<li>下面关闭<code>node2</code>,为添加一块20G的盘,测试其它<code>BlueStore</code>类型.<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">~$ ansible -i ../hosts node1 -b -m <span class="built_in">command</span> -a <span class="string">"ceph -s"</span></span><br><span class="line">192.168.99.101 | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">  cluster:</span><br><span class="line">    id:     0bf150da-b691-4382-bf3d-600e90c19fba</span><br><span class="line">    health: HEALTH_WARN</span><br><span class="line">            1/3 mons down, quorum node1,node3 <span class="comment"># 警告有一个节点shutdown.</span></span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum node1,node3, out of quorum: node2</span><br><span class="line">    mgr: node1(active), standbys: node3</span><br><span class="line">    osd: 1 osds: 1 up, 1 <span class="keyword">in</span></span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0B</span><br><span class="line">    usage:   1.00GiB used, 9.00GiB / 10.0GiB avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Parted-GPT-分区"><a href="#Parted-GPT-分区" class="headerlink" title="Parted(GPT 分区)"></a><code>Parted</code>(GPT 分区)</h3><ul>
<li><p>如果使用<code>fdisk(MBR)</code>分区会报错,下面使用<code>parted(GPT)</code>分区.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">root@node2:~<span class="comment"># parted /dev/sdb</span></span><br><span class="line">GNU Parted 3.2</span><br><span class="line">Using /dev/sdb</span><br><span class="line">Welcome to GNU Parted! Type <span class="string">'help'</span> to view a list of commands.</span><br><span class="line">(parted) mklabel gpt</span><br><span class="line">(parted) <span class="built_in">print</span></span><br><span class="line">Model: ATA VBOX HARDDISK (scsi)</span><br><span class="line">Disk /dev/sdb: 21.5GB</span><br><span class="line">Sector size (logical/physical): 512B/512B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags:</span><br><span class="line"></span><br><span class="line">Number  Start  End  Size  File system  Name  Flags</span><br><span class="line"></span><br><span class="line">(parted) mkpart parimary 0 10G</span><br><span class="line">Warning: The resulting partition is not properly aligned <span class="keyword">for</span> best performance.</span><br><span class="line">Ignore/Cancel?</span><br><span class="line">Ignore/Cancel? Ignore</span><br><span class="line">(parted) <span class="built_in">print</span></span><br><span class="line">Model: ATA VBOX HARDDISK (scsi)</span><br><span class="line">Disk /dev/sdb: 21.5GB</span><br><span class="line">Sector size (logical/physical): 512B/512B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags:</span><br><span class="line"></span><br><span class="line">Number  Start   End     Size     File system  Name      Flags</span><br><span class="line"> 1      17.4kB  10.0GB  10000MB               parimary</span><br><span class="line"></span><br><span class="line">(parted) mkpart parimary 10G 21.5G</span><br><span class="line">(parted) p</span><br><span class="line">Model: ATA VBOX HARDDISK (scsi)</span><br><span class="line">Disk /dev/sdb: 21.5GB</span><br><span class="line">Sector size (logical/physical): 512B/512B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags:</span><br><span class="line"></span><br><span class="line">Number  Start   End     Size     File system  Name      Flags</span><br><span class="line"> 1      17.4kB  10.0GB  10000MB               parimary</span><br><span class="line"> 2      10.0GB  21.5GB  11.5GB                parimary</span><br><span class="line">(parted) q</span><br><span class="line"></span><br><span class="line">root@node2:~<span class="comment"># partx /dev/sdb</span></span><br><span class="line">NR    START      END  SECTORS  SIZE NAME     UUID</span><br><span class="line"> 1       34 19531250 19531217  9.3G parimary a8c625b7-ebf2-4ceb-a9fd-5371dde59b35</span><br><span class="line"> 2 19531776 41940991 22409216 10.7G parimary 6463703f-c1f3-4ad7-8870-ed634db64131</span><br><span class="line"></span><br><span class="line">root@node2:~<span class="comment"># lsblk /dev/sdb</span></span><br><span class="line">NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sdb      8:16   0   20G  0 disk</span><br><span class="line">├─sdb1   8:17   0  9.3G  0 part</span><br><span class="line">└─sdb2   8:18   0 10.7G  0 part</span><br></pre></td></tr></table></figure>
</li>
<li><p>添加<code>osd.1</code>(block,block.db)</p>
</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">~$ ceph-deploy osd create node2 --data /dev/sdb2 --block-db /dev/sdb1</span><br><span class="line">[...]</span><br><span class="line">[node2][INFO  ] Running <span class="built_in">command</span>: sudo /usr/sbin/ceph-volume --cluster ceph lvm create --bluestore --data /dev/sdb2 --block.db /dev/sdb1</span><br><span class="line">[node2][DEBUG ] Running <span class="built_in">command</span>: /usr/bin/ceph-authtool --gen-print-key</span><br><span class="line">[node2][INFO  ] checking OSD status...</span><br><span class="line">[node2][INFO  ] Running <span class="built_in">command</span>: sudo /usr/bin/ceph --cluster=ceph osd <span class="built_in">stat</span> --format=json</span><br><span class="line">[ceph_deploy.osd][DEBUG ] Host node2 is now ready <span class="keyword">for</span> osd use.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看集群状态.</span></span><br><span class="line">~$ ansible -i ../hosts node1 -b -m <span class="built_in">command</span> -a <span class="string">"ceph -s"</span></span><br><span class="line">192.168.99.101 | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">  cluster:</span><br><span class="line">    id:     0bf150da-b691-4382-bf3d-600e90c19fba</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum node1,node2,node3</span><br><span class="line">    mgr: node1(active), standbys: node3, node2</span><br><span class="line">    osd: 2 osds: 2 up, 2 <span class="keyword">in</span></span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0B</span><br><span class="line">    usage:   2.00GiB used, 18.7GiB / 20.7GiB avail</span><br><span class="line">    pgs:</span><br><span class="line"><span class="comment"># 查看node1上机的ceph-1</span></span><br><span class="line">~$ ansible -i ../hosts node2 -b -m <span class="built_in">command</span> -a <span class="string">"ls -l /var/lib/ceph/osd/ceph-1"</span></span><br><span class="line">192.168.99.102 | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">total 48</span><br><span class="line">-rw-r--r-- 1 ceph ceph 393 May  9 23:53 activate.monmap</span><br><span class="line">lrwxrwxrwx 1 ceph ceph  93 May  9 23:53 block -&gt; /dev/ceph-98f53d51-8e74-4ca3-8b7a-87570c01733e/osd-block-f572ef53-805e-48ff-b936-da520e46be6b</span><br><span class="line">lrwxrwxrwx 1 ceph ceph   9 May  9 23:53 block.db -&gt; /dev/sdb1</span><br><span class="line">-rw-r--r-- 1 ceph ceph   2 May  9 23:53 bluefs</span><br><span class="line">-rw-r--r-- 1 ceph ceph  37 May  9 23:53 ceph_fsid</span><br><span class="line">-rw-r--r-- 1 ceph ceph  37 May  9 23:53 fsid</span><br><span class="line">-rw------- 1 ceph ceph  55 May  9 23:53 keyring</span><br><span class="line">-rw-r--r-- 1 ceph ceph   8 May  9 23:53 kv_backend</span><br><span class="line">-rw-r--r-- 1 ceph ceph  21 May  9 23:53 magic</span><br><span class="line">-rw-r--r-- 1 ceph ceph   4 May  9 23:53 mkfs_done</span><br><span class="line">-rw-r--r-- 1 ceph ceph  41 May  9 23:53 osd_key</span><br><span class="line">-rw-r--r-- 1 ceph ceph   6 May  9 23:53 ready</span><br><span class="line">-rw-r--r-- 1 ceph ceph  10 May  9 23:53 <span class="built_in">type</span></span><br><span class="line">-rw-r--r-- 1 ceph ceph   2 May  9 23:53 whoami</span><br></pre></td></tr></table></figure>
<h3 id="创建MDS服务器"><a href="#创建MDS服务器" class="headerlink" title="创建MDS服务器"></a>创建<code>MDS</code>服务器</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">~$ ceph-deploy mds create  FE001 DIG001</span><br><span class="line"><span class="comment"># 查看状态</span></span><br><span class="line">~$ ansible -i ../hosts node1 -b -m <span class="built_in">command</span> -a <span class="string">"ceph mds stat"</span></span><br><span class="line"></span><br><span class="line">~$ ansible -i ../hosts node1 -b -m <span class="built_in">command</span> -a <span class="string">"ceph osd pool create cephfs_data 64 64"</span></span><br><span class="line">pool <span class="string">'cephfs_data'</span> created</span><br><span class="line"></span><br><span class="line">~$ ansible -i ../hosts node1 -b -m <span class="built_in">command</span> -a <span class="string">"ceph osd pool create cephfs_metadata 64 64"</span></span><br><span class="line">pool <span class="string">'cephfs_metadata'</span> created</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建文件系统</span></span><br><span class="line">~$ ansible -i ../hosts node1 -b -m <span class="built_in">command</span> -a <span class="string">"ceph fs new cephfs cephfs_metadata cephfs_data"</span></span><br><span class="line">new fs with metadata pool 3 and data pool 2</span><br><span class="line"></span><br><span class="line">~$ ansible -i ../hosts node1 -b -m <span class="built_in">command</span> -a <span class="string">"ceph mds stat"</span></span><br><span class="line">cephfs-1/1/1 up  &#123;0=DIG001=up:active&#125;, 1 up:standby</span><br><span class="line">~$ ansible -i ../hosts node1 -b -m <span class="built_in">command</span> -a <span class="string">"ceph fs ls"</span></span><br><span class="line">name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</span><br><span class="line"></span><br><span class="line">~$ ansible -i ../hosts node1 -b -m <span class="built_in">command</span> -a <span class="string">"ceph fs status"</span></span><br><span class="line">cephfs - 0 clients</span><br><span class="line">======</span><br><span class="line">+------+--------+--------+---------------+-------+-------+</span><br><span class="line">| Rank | State  |  MDS   |    Activity   |  dns  |  inos |</span><br><span class="line">+------+--------+--------+---------------+-------+-------+</span><br><span class="line">|  0   | active | DIG001 | Reqs:    0 /s |   10  |   12  |</span><br><span class="line">+------+--------+--------+---------------+-------+-------+</span><br><span class="line">+-----------------+----------+-------+-------+</span><br><span class="line">|       Pool      |   <span class="built_in">type</span>   |  used | avail |</span><br><span class="line">+-----------------+----------+-------+-------+</span><br><span class="line">| cephfs_metadata | metadata | 2246  | 83.9G |</span><br><span class="line">|   cephfs_data   |   data   |    0  | 83.9G |</span><br><span class="line">+-----------------+----------+-------+-------+</span><br><span class="line"></span><br><span class="line">+-------------+</span><br><span class="line">| Standby MDS |</span><br><span class="line">+-------------+</span><br><span class="line">|    FE001    |</span><br><span class="line">+-------------+</span><br><span class="line">MDS version: ceph version 12.2.12 (1436006594665279fe734b4c15d7e08c13ebd777) luminous (stable)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看元数据.</span></span><br><span class="line">~$ sudo ceph osd metadata osd.2</span><br></pre></td></tr></table></figure>
<ul>
<li><p>挂载到文件系统，挂载文件系统，可以使用<code>/etc/ceph/ceph.client.admin.keyring</code>里的 key,也可以按照下面，新建一个用户与 key.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">~$ sudo ceph auth get-or-create client.cephfs mon <span class="string">'allow r'</span> mds <span class="string">'allow rw'</span> osd <span class="string">'allow rw pool=cephfs-data, allow rw pool=cephfs-metadata'</span></span><br><span class="line">~$ sudo ceph auth get client.cephfs</span><br><span class="line">exported keyring <span class="keyword">for</span> client.cephfs</span><br><span class="line">[client.cephfs]</span><br><span class="line">	key = AQDAwhldGXL3GhAAGsHu3XYUIwzS6z0SOcLMFA==</span><br><span class="line">	caps mds = <span class="string">"allow rw"</span></span><br><span class="line">	caps mon = <span class="string">"allow r"</span></span><br><span class="line">	caps osd = <span class="string">"allow rw pool=cephfs-data, allow rw pool=cephfs-metadata"</span></span><br><span class="line"></span><br><span class="line">~$ sudo mount.ceph node1:6789:/ /data -o name=cephfs,secret=AQDAwhldGXL3GhAAGsHu3XYUIwzS6z0SOcLMFA==</span><br></pre></td></tr></table></figure>
</li>
<li><p>上述的挂载的方式，会在<code>Shell</code>里看到<code>key</code>,不安全.可以把<code>AQDAwhldGXL3GhAAGsHu3XYUIwzS6z0SOcLMFA==</code>这个<code>Base64</code>的密钥字段保存成一个文件，加上<code>chmod 400</code>的权限.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">~$ sudo mount.ceph node1:6789:/ /data -o name=cephfs,secretfile=/etc/ceph/cephfs.secret</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入自动挂载</span></span><br><span class="line">~$ <span class="built_in">echo</span> <span class="string">"mon1:6789,mon2:6789,mon3:6789:/ /cephfs ceph name=cephfs,secretfile=/etc/ceph/cephfs.secret,_netdev,noatime 0 0"</span> | sudo tee -a /etc/fstab</span><br></pre></td></tr></table></figure>
</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[2019-07-01 14:20:45,567][ceph_volume.process][INFO ] stdout ceph.block_device=/dev/ceph-bd417a6a-cef6-4ff5-828a-5b68ec8843f0/osd-block-dcde5f54-c555-41ee-8c20-586f1069bcb7,ceph.block_uuid=wHZD0b-lU7P-vYFg-XOBI-zknV-Q181-0xKtt3,ceph.cephx_lockbox_secret=,ceph.cluster_fsid=d7f63adc-33d1-4ae9-9ba7-ae401950d965,ceph.cluster_name=ceph,ceph.crush_device_class=None,ceph.encrypted=0,ceph.osd_fsid=dcde5f54-c555-41ee-8c20-586f1069bcb7,ceph.osd_id=1,ceph.type=block,ceph.vdo=0<span class="string">";"</span>/dev/ceph-bd417a6a-cef6-4ff5-828a-5b68ec8843f0/osd-block-dcde5f54-c555-41ee-8c20-586f1069bcb7<span class="string">";"</span>osd-block-dcde5f54-c555-41ee-8c20-586f1069bcb7<span class="string">";"</span>ceph-bd417a6a-cef6-4ff5-828a-5b68ec8843f0<span class="string">";"</span>wHZD0b-lU7P-vYFg-XOBI-zknV-Q181-0xKtt3<span class="string">";"</span>60.00g</span><br><span class="line">[2019-07-01 14:20:45,567][ceph_volume][ERROR ] exception caught by decorator</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">File <span class="string">"/usr/lib/python2.7/dist-packages/ceph_volume/decorators.py"</span>, line 59, <span class="keyword">in</span> newfunc</span><br><span class="line"><span class="built_in">return</span> f(*a, \*\*kw)</span><br><span class="line">File <span class="string">"/usr/lib/python2.7/dist-packages/ceph_volume/main.py"</span>, line 148, <span class="keyword">in</span> main</span><br><span class="line">terminal.dispatch(self.mapper, subcommand_args)</span><br><span class="line">File <span class="string">"/usr/lib/python2.7/dist-packages/ceph_volume/terminal.py"</span>, line 182, <span class="keyword">in</span> dispatch</span><br><span class="line">instance.main()</span><br><span class="line">File <span class="string">"/usr/lib/python2.7/dist-packages/ceph_volume/devices/lvm/main.py"</span>, line 40, <span class="keyword">in</span> main</span><br><span class="line">terminal.dispatch(self.mapper, self.argv)</span><br><span class="line">File <span class="string">"/usr/lib/python2.7/dist-packages/ceph_volume/terminal.py"</span>, line 182, <span class="keyword">in</span> dispatch</span><br><span class="line">instance.main()</span><br><span class="line">File <span class="string">"/usr/lib/python2.7/dist-packages/ceph_volume/decorators.py"</span>, line 16, <span class="keyword">in</span> is_root</span><br><span class="line"><span class="built_in">return</span> func(*a, **kw)</span><br><span class="line">File <span class="string">"/usr/lib/python2.7/dist-packages/ceph_volume/devices/lvm/trigger.py"</span>, line 70, <span class="keyword">in</span> main</span><br><span class="line">Activate([<span class="string">'--auto-detect-objectstore'</span>, osd_id, osd_uuid]).main()</span><br><span class="line">File <span class="string">"/usr/lib/python2.7/dist-packages/ceph_volume/devices/lvm/activate.py"</span>, line 339, <span class="keyword">in</span> main</span><br><span class="line">self.activate(args)</span><br><span class="line">File <span class="string">"/usr/lib/python2.7/dist-packages/ceph_volume/decorators.py"</span>, line 16, <span class="keyword">in</span> is_root</span><br><span class="line"><span class="built_in">return</span> func(\*a, **kw)</span><br><span class="line">File <span class="string">"/usr/lib/python2.7/dist-packages/ceph_volume/devices/lvm/activate.py"</span>, line 249, <span class="keyword">in</span> activate</span><br><span class="line">raise RuntimeError(<span class="string">'could not find osd.%s with fsid %s'</span> % (osd_id, osd_fsid))</span><br><span class="line">RuntimeError: could not find osd.1 with fsid 3aeba7b7-f539-4b6a-afac-fc9fd62b90fa</span><br></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">~$ sudo lvs -o lv_tags</span><br><span class="line">LV Tags</span><br><span class="line"> ceph.block_device=/dev/ceph-9c0a0bae-d6db-498a-bf20-fe4cd8bdb3a9/osd-block-5c5a950b-8b36-4935-be8c-b59c24073874,ceph.block_uuid=yY970H-ztZ4-VtfA-2L9d-k3cF-Zi44-0i8MB1,ceph.cephx_lockbox_secret=,ceph.cluster_fsid=d7f63adc-33d1-4ae9-9ba7-ae401950d965,ceph.cluster_name=ceph,ceph.crush_device_class=None,ceph.encrypted=0,ceph.osd_fsid=5c5a950b-8b36-4935-be8c-b59c24073874,ceph.osd_id=2,ceph.type=block,ceph.vdo=0</span><br></pre></td></tr></table></figure>
<ul>
<li><p>可以在<code>Ceph</code>集群之外的服务器来安装<code>RGW</code>,需要安装<code>ceph-radosgw</code>包,如:<code>ceph-deploy install --rgw &lt;rgw-node&gt; [&lt;rgw-node&gt;...]</code>.下面为了方便起见,我直接在<code>node3,node4</code>上面安装<code>RGW</code>.</p>
</li>
<li><p>添加一个<code>mon</code>节点</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~$ ceph-deploy mon add node4</span><br></pre></td></tr></table></figure>
</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># node3在前面被管理过了.</span></span><br><span class="line">~$ ceph-deploy admin node4</span><br><span class="line">~$ ceph-deploy rgw create node3 node4</span><br><span class="line">[...]</span><br><span class="line">~$ ansible -i ../hosts node3 -b -m shell -a <span class="string">"ps -ef | grep rgw"</span></span><br><span class="line">192.168.99.103 | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">ceph        4272       1  0 02:05 ?        00:00:00 /usr/bin/radosgw -f --cluster ceph --name client.rgw.node3 --setuser ceph --setgroup ceph</span><br><span class="line">root        5040    5039  0 02:06 pts/0    00:00:00 /bin/sh -c ps -ef | grep rgw</span><br><span class="line">root        5042    5040  0 02:06 pts/0    00:00:00 grep rgw</span><br><span class="line"></span><br><span class="line">~$ ansible -i ../hosts node4 -b -m shell -a <span class="string">"ps -ef | grep rgw"</span></span><br><span class="line">192.168.99.104 | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">ceph        3411       1  0 02:05 ?        00:00:00 /usr/bin/radosgw -f --cluster ceph --name client.rgw.node4 --setuser ceph --setgroup ceph</span><br><span class="line">root        4211    4210  0 02:07 pts/0    00:00:00 /bin/sh -c ps -ef | grep rgw</span><br><span class="line">root        4213    4211  0 02:07 pts/0    00:00:00 grep rgw</span><br><span class="line"></span><br><span class="line"><span class="comment"># http 测试访问</span></span><br><span class="line">~$ curl node3:7480</span><br><span class="line">&lt;?xml version=<span class="string">"1.0"</span> encoding=<span class="string">"UTF-8"</span>?&gt;&lt;ListAllMyBucketsResult xmlns=<span class="string">"http://s3.amazonaws.com/doc/2006-03-01/"</span>&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>修改 RGW 的默认端口.在 ceph.conf 加入下面两行</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[client.rgw.node4]</span><br><span class="line"><span class="comment"># rgw_frontends = "civetweb port=80"</span></span><br><span class="line">rgw_frontends = civetweb port=80+443s ssl_certificate=/etc/ceph/private/keyandcert.pem</span><br></pre></td></tr></table></figure>
<ul>
<li>上传当前目录下的配置文件到指定的在节点上去.<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">~$ ceph-deploy --overwrite-conf config push node4</span><br><span class="line">~$ ansible -i hosts node4 -b  -m systemd -a <span class="string">"name=radosgw state=restarted daemon_reload=yes"</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="civetweb配置"><a href="#civetweb配置" class="headerlink" title="civetweb配置"></a><code>civetweb</code>配置</h3><ul>
<li><p>如果不是通过<code>ceph-deploy</code>部署的集群需要通过下面的流程,手动配置添加<code>RGW</code></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建keyring</span></span><br><span class="line">~$ sudo ceph-authtool --create-keyring /etc/ceph/ceph.client.radosgw.keyring</span><br><span class="line"><span class="comment"># 生成密钥匙</span></span><br><span class="line">~$ sudo ceph-authtool /etc/ceph/ceph.client.radosgw.keyring -n client.rgw.node3 --gen-key</span><br><span class="line"><span class="comment">#　设置权限</span></span><br><span class="line">~$ sudo ceph-authtool -n client.rgw.node3 --<span class="built_in">cap</span> osd <span class="string">'allow rwx'</span> --<span class="built_in">cap</span> mon <span class="string">'allow rwx'</span> /etc/ceph/ceph.client.radosgw.keyring</span><br><span class="line"><span class="comment"># 导入keyring到集群中</span></span><br><span class="line">~$ sudo ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.rgw.node3 -i /etc/ceph/ceph.client.radosgw.keyring</span><br><span class="line">~$ cat /etc/ceph/ceph.conf</span><br><span class="line">[...]</span><br><span class="line">[client.rgw.node3]</span><br><span class="line">rgw_frontends = civetweb port=80</span><br><span class="line">host=node3</span><br><span class="line">rgw_s3_auth_use_keystone=<span class="literal">false</span></span><br><span class="line">keyring=/etc/ceph/ceph.client.radosgw.keyring</span><br><span class="line"><span class="built_in">log</span> file=/var/<span class="built_in">log</span>/ceph/client.radosgw.gateway.log</span><br></pre></td></tr></table></figure>
</li>
<li><p>这里是通过<code>ceph-deploy</code>部署的,只需导出相应到<code>/etc/ceph/ceph.client.radosgw.keyring</code></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">~$ sudo ceph auth get client.rgw.node3</span><br><span class="line">exported keyring <span class="keyword">for</span> client.rgw.node3</span><br><span class="line"><span class="comment"># 把下面这行复制,并创建到 /etc/ceph/ceph.client.radosgw.keyring</span></span><br><span class="line">[client.rgw.node3]</span><br><span class="line">	key = AQC8FNVcl07ALRAAfhr+APpuKW/VvknEzD7hpg==</span><br><span class="line">	caps mon = <span class="string">"allow rw"</span></span><br><span class="line">	caps osd = <span class="string">"allow rwx"</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>测试访问</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~$ sudo radosgw --cluster ceph --name client.rgw.node3 --setuser ceph --setgroup ceph -d --debug_ms 1 --keyring /etc/ceph/ceph.client.radosgw.keyring</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果一切正常开启,就使用<code>systemctl restart ceph-radosgw@rgw.node3</code>重启它的服务,如果服务有错,使用<code>journalctl -u ceph-radosgw@rgw.node3</code>查看.</p>
</li>
</ul>
<h2 id="客户端访问"><a href="#客户端访问" class="headerlink" title="客户端访问"></a>客户端访问</h2><h3 id="创建S3用户"><a href="#创建S3用户" class="headerlink" title="创建S3用户"></a>创建<code>S3</code>用户</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">$ ansible -i ../hosts node4 -b -m <span class="built_in">command</span> -a <span class="string">"radosgw-admin user create --uid=\"lcy\" --display-name=\"admin user test\""</span></span><br><span class="line">192.168.99.104 | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"user_id"</span>: <span class="string">"lcy"</span>,</span><br><span class="line">    <span class="string">"display_name"</span>: <span class="string">"admin user test"</span>,</span><br><span class="line">    <span class="string">"email"</span>: <span class="string">""</span>,</span><br><span class="line">    <span class="string">"suspended"</span>: 0,</span><br><span class="line">    <span class="string">"max_buckets"</span>: 1000,</span><br><span class="line">    <span class="string">"auid"</span>: 0,</span><br><span class="line">    <span class="string">"subusers"</span>: [],</span><br><span class="line">    <span class="string">"keys"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"user"</span>: <span class="string">"lcy"</span>,</span><br><span class="line">            <span class="string">"access_key"</span>: <span class="string">"74I2DQ89N5EL1OGCCSCV"</span>,   <span class="comment"># s3cmd必须提供</span></span><br><span class="line">            <span class="string">"secret_key"</span>: <span class="string">"ePz9ONOrZS4BB8RN44KBYxCzRA0UNz8Kyu5kXzvE"</span> <span class="comment"># s3cmd必须提供</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"swift_keys"</span>: [],</span><br><span class="line">    <span class="string">"caps"</span>: [],</span><br><span class="line">    <span class="string">"op_mask"</span>: <span class="string">"read, write, delete"</span>,</span><br><span class="line">    <span class="string">"default_placement"</span>: <span class="string">""</span>,</span><br><span class="line">    <span class="string">"placement_tags"</span>: [],</span><br><span class="line">    <span class="string">"bucket_quota"</span>: &#123;</span><br><span class="line">        <span class="string">"enabled"</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">"check_on_raw"</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">"max_size"</span>: -1,</span><br><span class="line">        <span class="string">"max_size_kb"</span>: 0,</span><br><span class="line">        <span class="string">"max_objects"</span>: -1</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"user_quota"</span>: &#123;</span><br><span class="line">        <span class="string">"enabled"</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">"check_on_raw"</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">"max_size"</span>: -1,</span><br><span class="line">        <span class="string">"max_size_kb"</span>: 0,</span><br><span class="line">        <span class="string">"max_objects"</span>: -1</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"temp_url_keys"</span>: [],</span><br><span class="line">    <span class="string">"type"</span>: <span class="string">"rgw"</span></span><br><span class="line">&#125;</span><br><span class="line">~$ ansible -i ../hosts node4 -b -m <span class="built_in">command</span> -a <span class="string">"radosgw-admin user list"</span></span><br><span class="line">192.168.99.104 | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">[</span><br><span class="line">    <span class="string">"testuser"</span>,</span><br><span class="line">    <span class="string">"lcy"</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<h3 id="创建Swift用户"><a href="#创建Swift用户" class="headerlink" title="创建Swift用户"></a>创建<code>Swift</code>用户</h3><ul>
<li><code>Swift</code>用户是作为子用户被创建,因此要先创建用户,如下:<code>lcy</code><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">~$ ansible -i ../hosts node4 -b -m <span class="built_in">command</span> -a <span class="string">"radosgw-admin subuser create --uid=lcy --subuser=lcy:swift --access=full"</span></span><br><span class="line">192.168.99.104 | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"user_id"</span>: <span class="string">"lcy"</span>,</span><br><span class="line">    <span class="string">"display_name"</span>: <span class="string">"admin user test"</span>,</span><br><span class="line">    <span class="string">"email"</span>: <span class="string">""</span>,</span><br><span class="line">    <span class="string">"suspended"</span>: 0,</span><br><span class="line">    <span class="string">"max_buckets"</span>: 1000,</span><br><span class="line">    <span class="string">"auid"</span>: 0,</span><br><span class="line">    <span class="string">"subusers"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"id"</span>: <span class="string">"lcy:swift"</span>,</span><br><span class="line">            <span class="string">"permissions"</span>: <span class="string">"full-control"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"keys"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"user"</span>: <span class="string">"lcy"</span>,</span><br><span class="line">            <span class="string">"access_key"</span>: <span class="string">"74I2DQ89N5EL1OGCCSCV"</span>,</span><br><span class="line">            <span class="string">"secret_key"</span>: <span class="string">"ePz9ONOrZS4BB8RN44KBYxCzRA0UNz8Kyu5kXzvE"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"swift_keys"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"user"</span>: <span class="string">"lcy:swift"</span>,</span><br><span class="line">            <span class="string">"secret_key"</span>: <span class="string">"bw2zByEnhZMzpSvrb9tYi5rjOT8mK69SkuuWFN8j"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"caps"</span>: [],</span><br><span class="line">    <span class="string">"op_mask"</span>: <span class="string">"read, write, delete"</span>,</span><br><span class="line">    <span class="string">"default_placement"</span>: <span class="string">""</span>,</span><br><span class="line">    <span class="string">"placement_tags"</span>: [],</span><br><span class="line">    <span class="string">"bucket_quota"</span>: &#123;</span><br><span class="line">        <span class="string">"enabled"</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">"check_on_raw"</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">"max_size"</span>: -1,</span><br><span class="line">        <span class="string">"max_size_kb"</span>: 0,</span><br><span class="line">        <span class="string">"max_objects"</span>: -1</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"user_quota"</span>: &#123;</span><br><span class="line">        <span class="string">"enabled"</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">"check_on_raw"</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">"max_size"</span>: -1,</span><br><span class="line">        <span class="string">"max_size_kb"</span>: 0,</span><br><span class="line">        <span class="string">"max_objects"</span>: -1</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"temp_url_keys"</span>: [],</span><br><span class="line">    <span class="string">"type"</span>: <span class="string">"rgw"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="使用Python客户端库测试"><a href="#使用Python客户端库测试" class="headerlink" title="使用Python客户端库测试"></a>使用<code>Python</code>客户端库测试</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">~$ pip install boto python-swiftclient</span><br><span class="line">~$ ipython</span><br><span class="line">In [1]: access_key = <span class="string">'74I2DQ89N5EL1OGCCSCV'</span></span><br><span class="line">In [2]: secret_key = <span class="string">'ePz9ONOrZS4BB8RN44KBYxCzRA0UNz8Kyu5kXzvE'</span></span><br><span class="line">In [3]: import boto.s3.connection</span><br><span class="line">In [4]: conn = boto.connect_s3(aws_access_key_id=access_key,aws_secret_access_key=secret_key,host=<span class="string">'192.168.99.103'</span>,port=7480,is_secure=False,calling_format=boto.s3.connection.OrdinaryCallingFormat())</span><br><span class="line">In [5]: bkt = conn.create_bucket(<span class="string">'ooo-bucket'</span>)</span><br><span class="line">In [6]: <span class="keyword">for</span> bkt <span class="keyword">in</span> conn.get_all_buckets():</span><br><span class="line">   ...:     <span class="built_in">print</span>(<span class="string">"&#123;name&#125; &#123;created&#125;"</span>.format(name=bkt.name,created=bkt.creation_date))</span><br><span class="line">   ...:                                                                                                                                                                   <span class="comment"># 创建并获取成功.</span></span><br><span class="line">ooo-bucket 2019-05-10T07:08:26.456Z</span><br><span class="line"><span class="comment"># 使用swift client 测试.</span></span><br><span class="line">~$ swift -A http://node4:7480/auth/1.0 -U lcy:swift -K <span class="string">'bw2zByEnhZMzpSvrb9tYi5rjOT8mK69SkuuWFN8j'</span> list</span><br><span class="line">ooo-bucket</span><br></pre></td></tr></table></figure>
<h3 id="使用s3cmd测试"><a href="#使用s3cmd测试" class="headerlink" title="使用s3cmd测试"></a>使用<code>s3cmd</code>测试</h3><ul>
<li><p>使用<code>s3cmd</code>之前,需要先使用<code>s3cmd --configure</code>交互设置好相应的参数.这里跳过直接写入一些必要的连接参数.这里可以配置任一<code>rgw(node3,node4)</code>节点测试.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">~$ sudo apt instal s3cmd</span><br><span class="line">~$ cat &lt;&lt;EOF &gt; ~/.s3cfg</span><br><span class="line">[default]</span><br><span class="line">access_key = 74I2DQ89N5EL1OGCCSCV</span><br><span class="line">host_base = node3:7480</span><br><span class="line">host_bucket = node3:7480/%(bucket)</span><br><span class="line">secret_key = ePz9ONOrZS4BB8RN44KBYxCzRA0UNz8Kyu5kXzvE</span><br><span class="line">cloudfront_host = node3:7480</span><br><span class="line">use_https = False</span><br><span class="line">bucket_location = US</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出所有桶</span></span><br><span class="line">~$ s3cmd ls</span><br><span class="line">2019-05-10 07:08  s3://ooo-bucket</span><br><span class="line"><span class="comment"># 创建桶</span></span><br><span class="line">~$ s3cmd mb s3://sql</span><br><span class="line"><span class="comment"># 上传文件进桶</span></span><br><span class="line">~$ s3cmd put ~/wxdb-20190422-1638.sql  s3://sql</span><br><span class="line">upload: <span class="string">'/home/lcy/wxdb-20190422-1638.sql'</span> -&gt; <span class="string">'s3://sql/wxdb-20190422-1638.sql'</span>  [1 of 1]</span><br><span class="line"> 197980 of 197980   100% <span class="keyword">in</span>    1s   104.33 kB/s  <span class="keyword">done</span></span><br><span class="line"><span class="comment"># 列出桶里的文件</span></span><br><span class="line">~$ s3cmd ls s3://sql</span><br><span class="line">2019-05-10 08:12    197980   s3://sql/wxdb-20190422-1638.sql</span><br><span class="line"><span class="comment"># 下载桶里的文件到本地.</span></span><br><span class="line">~$ s3cmd get s3://sql/wxdb-20190422-1638.sql</span><br><span class="line">download: <span class="string">'s3://sql/wxdb-20190422-1638.sql'</span> -&gt; <span class="string">'./wxdb-20190422-1638.sql'</span>  [1 of 1]</span><br><span class="line"> 197980 of 197980   100% <span class="keyword">in</span>    0s    57.23 MB/s  <span class="keyword">done</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>查看集群利用率统计</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">~$ ansible -i ../hosts node2 -b -m <span class="built_in">command</span> -a <span class="string">"ceph df"</span></span><br><span class="line">192.168.99.102 | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">GLOBAL:</span><br><span class="line">    SIZE        AVAIL       RAW USED     %RAW USED</span><br><span class="line">    20.7GiB     18.7GiB      2.01GiB          9.72</span><br><span class="line">POOLS:</span><br><span class="line">    NAME                          ID     USED        %USED     MAX AVAIL     OBJECTS</span><br><span class="line">    .rgw.root                     1      2.08KiB         0       5.83GiB           6</span><br><span class="line">    default.rgw.control           2           0B         0       5.83GiB           8</span><br><span class="line">    default.rgw.meta              3      2.13KiB         0       5.83GiB          12</span><br><span class="line">    default.rgw.log               4           0B         0       5.83GiB         207</span><br><span class="line">    default.rgw.buckets.index     5           0B         0       5.83GiB           3</span><br><span class="line">    default.rgw.buckets.data      6       193KiB         0       5.83GiB           1</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="s3fs-fuse挂载文件系统"><a href="#s3fs-fuse挂载文件系统" class="headerlink" title="s3fs-fuse挂载文件系统"></a><code>s3fs-fuse</code>挂载文件系统</h3><ul>
<li><a href="https://github.com/s3fs-fuse/s3fs-fuse" target="_blank" rel="noopener">s3fs-fuse</a></li>
<li><a href="https://testdriven.io/blog/storing-django-static-and-media-files-on-amazon-s3/" target="_blank" rel="noopener">Django 使用 AWS S3 存储文件参考</a></li>
<li><p>原本想直接把<code>ceph s3 bucket</code>做为一个卷挂到 docker 上面,暂时没试验成功.下是如在的宿主机里挂载,再通过<code>-v</code>挂到 docker 上.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">~$ sudo apt install s3fs fuse</span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可把它放在/etc/passwd-s3fs</span></span><br><span class="line">~$ <span class="built_in">echo</span> ACCESS_KEY_ID:SECRET_ACCESS_KEY &gt; <span class="variable">$&#123;HOME&#125;</span>/.passwd-s3fs &amp;&amp; chmod 600 <span class="variable">$&#123;HOME&#125;</span>/.passwd-s3fs</span><br><span class="line">~$ s3cmd ls</span><br><span class="line">2019-05-10 08:10  s3://iso</span><br><span class="line">2019-05-16 03:50  s3://media  <span class="comment"># 下面将把它挂载成一个文件目录.</span></span><br><span class="line">2019-05-10 07:08  s3://ooo-bucket</span><br><span class="line">2019-05-16 06:44  s3://public</span><br><span class="line">2019-05-10 08:10  s3://sql</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里需注,ceph s3 必需使用use_path_request_style参数,因为它不是AWS原生的.</span></span><br><span class="line">~$ s3fs media /data/s3fs -o allow_other,<span class="built_in">umask</span>=022,use_path_request_style,url=http://node3</span><br><span class="line"></span><br><span class="line">~$ df -h | grep s3fs</span><br><span class="line">s3fs            256T     0  256T   0% /data/s3fs</span><br><span class="line"></span><br><span class="line">~$ grep s3fs /etc/mtab</span><br><span class="line">s3fs /data/s3fs fuse.s3fs rw,nosuid,nodev,relatime,user_id=1000,group_id=120,allow_other 0 0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如是挂载不加umask,默认是0000,无访问权限.</span></span><br><span class="line">~$ ls -l /data/s3fs/</span><br><span class="line">total 9397</span><br><span class="line">drwxr-xr-x 1 root root       0 Jan  1  1970 hls</span><br><span class="line">-rwxr-xr-x 1 root root 3100721 May 16 14:24 video.mp4</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果需要调试问题,可加入<code>-o dbglevel=info -f -o curldbg</code>启动,具体还有其它功能,可以详查看它的<a href="https://github.com/s3fs-fuse/s3fs-fuse" target="_blank" rel="noopener">github</a>以及它的帮助命令.</p>
</li>
</ul>
<h2 id="警告错误类"><a href="#警告错误类" class="headerlink" title="警告错误类"></a>警告错误类</h2><ul>
<li><p><a href="https://blog.didiyun.com/index.php/2018/11/08/didiyun-ceph-pg/" target="_blank" rel="noopener">参考文档</a></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ ansible -i ../hosts node1 -b -m <span class="built_in">command</span> -a <span class="string">"ceph -s"</span></span><br><span class="line">192.168.99.101 | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">  cluster:</span><br><span class="line">    id:     0bf150da-b691-4382-bf3d-600e90c19fba</span><br><span class="line">    health: HEALTH_WARN</span><br><span class="line">            Degraded data redundancy: 237/711 objects degraded (33.333%), 27 pgs degraded, 48 pgs undersized</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 4 daemons, quorum node1,node2,node3,node4</span><br><span class="line">    mgr: node1(active), standbys: node2, node3</span><br><span class="line">    osd: 2 osds: 2 up, 2 <span class="keyword">in</span></span><br><span class="line">    rgw: 2 daemons active</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   6 pools, 48 pgs</span><br><span class="line">    objects: 237 objects, 198KiB</span><br><span class="line">    usage:   2.01GiB used, 18.7GiB / 20.7GiB avail</span><br><span class="line">    pgs:     237/711 objects degraded (33.333%)</span><br><span class="line">             27 active+undersized+degraded</span><br><span class="line">             21 active+undersized</span><br></pre></td></tr></table></figure>
</li>
<li><p>根据上面的警告,数据中的<code>pg</code>降级,重启<code>OSD</code>的节点服务<code>systemctl restart ceph-osd.target</code>之后再看.</p>
</li>
<li>后来仔细查看发现,是因为<code>osd</code>的<code>备份数量是3</code>,而我这里只创建了两个<code>osd</code>,所以才会出现上述降级警告.可以修改<code>备份数量为2</code>,也可以再增加一个<code>osd</code>节点.</li>
<li>下面也参照<code>node2</code>一样添加一个 20G 的盘,分成两个区,使用(block,block.wal)方式创建.</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">~$ ceph-deploy osd create node3 --data /dev/sdb2 --block-wal /dev/sdb1</span><br><span class="line"></span><br><span class="line">~$ ansible -i ../hosts node1 -b -m <span class="built_in">command</span> -a <span class="string">"ceph osd tree"</span></span><br><span class="line">192.168.99.101 | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">ID CLASS WEIGHT  TYPE NAME      STATUS REWEIGHT PRI-AFF</span><br><span class="line">-1       0.03058 root default</span><br><span class="line">-3       0.00980     host node1</span><br><span class="line"> 0   hdd 0.00980         osd.0      up  1.00000 1.00000</span><br><span class="line">-5       0.01039     host node2</span><br><span class="line"> 1   hdd 0.01039         osd.1      up  1.00000 1.00000</span><br><span class="line">-7       0.01039     host node3</span><br><span class="line"> 2   hdd 0.01039         osd.2      up  1.00000 1.00000</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再次查看,状态已经正常了.</span></span><br><span class="line">~$ ansible -i ../hosts node1 -b -m <span class="built_in">command</span> -a <span class="string">"ceph health"</span></span><br><span class="line">192.168.99.101 | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">HEALTH_OK</span><br></pre></td></tr></table></figure>
<ul>
<li>Ceph: HEALTH_WARN clock skew detected</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把所有节点的ntp默认开机启动.</span></span><br><span class="line">~$ ansible -i hosts all -b -m systemd -a <span class="string">"name=ntp enabled=yes state=started"</span></span><br></pre></td></tr></table></figure>
<ul>
<li>application not enabled on 1 pool(s) 警告处理</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">~$ sudo ceph health detail</span><br><span class="line">HEALTH_WARN application not enabled on 1 pool(s)</span><br><span class="line">POOL_APP_NOT_ENABLED application not enabled on 1 pool(s)</span><br><span class="line">    application not enabled on pool <span class="string">'kube'</span></span><br><span class="line">    use <span class="string">'ceph osd pool application enable &lt;pool-name&gt; &lt;app-name&gt;'</span>, <span class="built_in">where</span> &lt;app-name&gt; is <span class="string">'cephfs'</span>, <span class="string">'rbd'</span>, <span class="string">'rgw'</span>, or freeform <span class="keyword">for</span> custom applications.</span><br><span class="line">~$ sudo ceph osd pool application <span class="built_in">enable</span> kube rbd</span><br><span class="line">enabled application <span class="string">'rbd'</span> on pool <span class="string">'kube'</span></span><br></pre></td></tr></table></figure>
<ul>
<li>安装完成后各节点的服务列表如下:<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">$ ansible -i hosts node -b -m <span class="built_in">command</span> -a <span class="string">"netstat -tnlp"</span></span><br><span class="line">192.168.99.102 | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name</span><br><span class="line">tcp        0      0 192.168.99.102:6789     0.0.0.0:*               LISTEN      476/ceph-mon</span><br><span class="line">tcp        0      0 192.168.99.102:6800     0.0.0.0:*               LISTEN      875/ceph-osd</span><br><span class="line">tcp        0      0 192.168.99.102:6801     0.0.0.0:*               LISTEN      875/ceph-osd</span><br><span class="line">tcp        0      0 192.168.99.102:6802     0.0.0.0:*               LISTEN      875/ceph-osd</span><br><span class="line">tcp        0      0 192.168.99.102:6803     0.0.0.0:*               LISTEN      875/ceph-osd</span><br><span class="line">tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      529/sshd</span><br><span class="line">tcp6       0      0 :::22                   :::*                    LISTEN      529/sshd</span><br><span class="line"></span><br><span class="line">192.168.99.101 | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name</span><br><span class="line">tcp        0      0 192.168.99.101:6789     0.0.0.0:*               LISTEN      480/ceph-mon</span><br><span class="line">tcp        0      0 192.168.99.101:6800     0.0.0.0:*               LISTEN      1015/ceph-osd</span><br><span class="line">tcp        0      0 192.168.99.101:6801     0.0.0.0:*               LISTEN      1015/ceph-osd</span><br><span class="line">tcp        0      0 192.168.99.101:6802     0.0.0.0:*               LISTEN      1015/ceph-osd</span><br><span class="line">tcp        0      0 192.168.99.101:6803     0.0.0.0:*               LISTEN      1015/ceph-osd</span><br><span class="line">tcp        0      0 192.168.99.101:6804     0.0.0.0:*               LISTEN      476/ceph-mgr</span><br><span class="line">tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      537/sshd</span><br><span class="line">tcp6       0      0 :::22                   :::*                    LISTEN      537/sshd</span><br><span class="line"></span><br><span class="line">192.168.99.103 | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name</span><br><span class="line">tcp        0      0 192.168.99.103:6789     0.0.0.0:*               LISTEN      479/ceph-mon</span><br><span class="line">tcp        0      0 192.168.99.103:6800     0.0.0.0:*               LISTEN      965/ceph-osd</span><br><span class="line">tcp        0      0 192.168.99.103:6801     0.0.0.0:*               LISTEN      965/ceph-osd</span><br><span class="line">tcp        0      0 192.168.99.103:6802     0.0.0.0:*               LISTEN      965/ceph-osd</span><br><span class="line">tcp        0      0 192.168.99.103:6803     0.0.0.0:*               LISTEN      965/ceph-osd</span><br><span class="line">tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      527/sshd</span><br><span class="line">tcp        0      0 0.0.0.0:7480            0.0.0.0:*               LISTEN      480/radosgw</span><br><span class="line">tcp6       0      0 :::22                   :::*                    LISTEN      527/sshd</span><br><span class="line"></span><br><span class="line">192.168.99.104 | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name</span><br><span class="line">tcp        0      0 192.168.99.104:6789     0.0.0.0:*               LISTEN      445/ceph-mon</span><br><span class="line">tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      447/radosgw</span><br><span class="line">tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      515/sshd</span><br><span class="line">tcp6       0      0 :::22                   :::*                    LISTEN      515/sshd</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="与Kubernetes集成"><a href="#与Kubernetes集成" class="headerlink" title="与Kubernetes集成"></a>与<code>Kubernetes</code>集成</h1><ul>
<li><a href="https://akomljen.com/using-existing-ceph-cluster-for-kubernetes-persistent-storage/" target="_blank" rel="noopener">Using Existing Ceph Cluster for kubernetes Persistent Storage</a></li>
</ul>
<h2 id="创建RBD"><a href="#创建RBD" class="headerlink" title="创建RBD"></a>创建<code>RBD</code></h2><ul>
<li><p>操作<code>RBD</code>必须直接登录到服务器里操作,<code>ceph-deploy</code>没有提供相关的接口.可以使用<code>Ansible</code>进行远程指操作.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#　关于如何计算池的pg数,可以参考　https://ceph.com/pgcalc/</span></span><br><span class="line">~$ sudo ceph  osd pool create kube 64 64</span><br><span class="line">pool <span class="string">'kube'</span> created</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置存储池的副本数</span></span><br><span class="line">~$ sudo ceph osd pool <span class="built_in">set</span> kube size 2</span><br><span class="line"></span><br><span class="line">~$ sudo ceph osd lspools</span><br><span class="line">1 .rgw.root,2 default.rgw.control,3 default.rgw.meta,4 default.rgw.log,5 default.rgw.buckets.index,6 default.rgw.buckets.data,7 volumes,8 kube,</span><br><span class="line"></span><br><span class="line">~$ sudo rbd create kube/cephimage2 --size 40960</span><br><span class="line">~$ sudo rbd list kube</span><br><span class="line">cephimage2</span><br><span class="line"></span><br><span class="line">~$ sudo rbd info kube/cephimage2</span><br><span class="line">rbd image <span class="string">'cephimage2'</span>:</span><br><span class="line">        size 40GiB <span class="keyword">in</span> 10240 objects</span><br><span class="line">        order 22 (4MiB objects)</span><br><span class="line">        block_name_prefix: rbd_data.519a06b8b4567</span><br><span class="line">        format: 2</span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten</span><br><span class="line">        flags:</span><br><span class="line">        create_timestamp: Mon May 13 01:44:35 2019</span><br><span class="line"></span><br><span class="line">~$ sudo rbd create kube/cephimage1 --size 10240</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把 cepimage1原来10G大小,扩展至20G</span></span><br><span class="line">~$ sudo rbd resize kube/cephimage1 --size 20480</span><br><span class="line"></span><br><span class="line">-$ sudo rbd create kube/cephimage3 --size 4096 --image-feature layering</span><br></pre></td></tr></table></figure>
</li>
<li><p>默认创建<code>RBD</code>的会开启<code>(layering, exclusive-lock, object-map, fast-diff, deep-flatten)</code>特性,低版本的<code>linux kernel</code>会不支持,一般低版本仅支持<code>layering</code>特性.如果内核版本过低,创建<code>Pod</code>时会出现下面的要错误.</p>
</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MountVolume.WaitForAttach failed <span class="keyword">for</span> volume <span class="string">"ceph-rbd-pv"</span> : rbd: map failed <span class="built_in">exit</span> status 6, rbd output: rbd: sysfs write failed RBD image feature <span class="built_in">set</span> mismatch. Try disabling features unsupported by the kernel with <span class="string">"rbd feature disable"</span>. In some cases useful info is found <span class="keyword">in</span> syslog - try <span class="string">"dmesg | tail"</span>. rbd: map failed: (6) No such device or address</span><br><span class="line"></span><br><span class="line">~<span class="comment"># dmesg</span></span><br><span class="line">[1355258.253726] rbd: image foo: image uses unsupported features: 0x38</span><br></pre></td></tr></table></figure>
<ul>
<li><p>创建集群<code>Pod</code></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">~$ git <span class="built_in">clone</span> https://github.com/kubernetes/examples.git</span><br><span class="line">~$ <span class="built_in">cd</span> examples/staging/volumes/rbd/</span><br><span class="line">~$ tree</span><br><span class="line">.</span><br><span class="line">├── rbd-with-secret.yaml</span><br><span class="line">├── rbd.yaml</span><br><span class="line">├── README.md</span><br><span class="line">└── secret</span><br><span class="line">    └── ceph-secret.yaml</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改<code>rbd-with-secret.yaml</code>的内容如下:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: rbd2</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - image: busybox</span><br><span class="line">      <span class="built_in">command</span>: [<span class="string">"sleep"</span>, <span class="string">"60000"</span>]</span><br><span class="line">      name: rbd-rw</span><br><span class="line">      volumeMounts:</span><br><span class="line">      - name: rbdpd</span><br><span class="line">        mountPath: /mnt/rbd</span><br><span class="line">  volumes:</span><br><span class="line">    - name: rbdpd</span><br><span class="line">      rbd:</span><br><span class="line">        monitors:</span><br><span class="line">        - <span class="string">'192.168.99.101:6789'</span></span><br><span class="line">        - <span class="string">'192.168.99.102:6789'</span></span><br><span class="line">        - <span class="string">'192.168.99.103:6789'</span></span><br><span class="line">        - <span class="string">'192.168.99.104:6789'</span></span><br><span class="line">        pool: kube</span><br><span class="line">        image: cephimage3</span><br><span class="line">        fsType: ext4</span><br><span class="line">        readOnly: <span class="literal">false</span></span><br><span class="line">        user: admin</span><br><span class="line">        secretRef:</span><br><span class="line">          name: ceph-secret</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改<code>ceph-secret</code>,注意替换文件里的<code>key</code>字段.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">~$ ansible -i hosts node1 -b -m <span class="built_in">command</span> -a <span class="string">"cat /etc/ceph/ceph.client.admin.keyring"</span> | grep key | awk <span class="string">'&#123;printf "%s",$NF&#125;'</span> | base64</span><br><span class="line">QVFESDB0UmNFSStwR3hBQUJ4aW1ZT1VXRWVTckdzSStpZklCOWc9PQ==</span><br><span class="line">~$ cat secret/ceph-secret.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: ceph-secret</span><br><span class="line"><span class="built_in">type</span>: <span class="string">"kubernetes.io/rbd"</span></span><br><span class="line">data:</span><br><span class="line">  key: QVFESDB0UmNFSStwR3hBQUJ4aW1ZT1VXRWVTckdzSStpZklCOWc9PQ==    <span class="comment"># 来源于上面的命令输出.</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>创建<code>Pod</code>与<code>Secret</code></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">~$ kubectl create -f secret/ceph-secret.yaml</span><br><span class="line">~$ kubectl create -f rbd-with-secret</span><br><span class="line"></span><br><span class="line">~$ kubectl get pods -o wide</span><br><span class="line">NAME      READY   STATUS    RESTARTS   AGE   IP           NODE    NOMINATED NODE   READINESS GATES</span><br><span class="line">rbd2      1/1     Running   0          60m   10.244.1.2   node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">~$ kubectl get secret</span><br><span class="line">NAME                  TYPE                                  DATA   AGE</span><br><span class="line">ceph-secret           kubernetes.io/rbd                     1      17h</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这样就像使用本的盘一样了.</span></span><br><span class="line">~$ kubectl <span class="built_in">exec</span> -it rbd2 -- df -h | grep -e <span class="string">"rbd0"</span> -e <span class="string">"secret"</span></span><br><span class="line">/dev/rbd0                 3.9G     16.0M      3.8G   0% /mnt/rbd</span><br><span class="line">tmpfs                   498.2M     12.0K    498.2M   0% /var/run/secrets/kubernetes.io/serviceaccount</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建基于<code>RBD</code>的<code>PV</code>以及<code>PVC</code>测试</p>
</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">~$ cat rbd-pv.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: ceph-rbd-pv</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 4Gi</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  rbd:</span><br><span class="line">    monitors:</span><br><span class="line">    - <span class="string">'192.168.99.101:6789'</span></span><br><span class="line">    - <span class="string">'192.168.99.102:6789'</span></span><br><span class="line">    - <span class="string">'192.168.99.103:6789'</span></span><br><span class="line">    - <span class="string">'192.168.99.104:6789'</span></span><br><span class="line">    pool: kube</span><br><span class="line">    image: cephimage1</span><br><span class="line">    fsType: ext4</span><br><span class="line">    readOnly: <span class="literal">false</span></span><br><span class="line">    user: admin</span><br><span class="line">    secretRef:</span><br><span class="line">      name: ceph-secret</span><br><span class="line">  persistentVolumeReclaimPolicy: Recycle</span><br><span class="line"></span><br><span class="line">~$ cat rbd-pvc.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: ceph-rbd-pvc</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 4Gi</span><br><span class="line"></span><br><span class="line">~$ kubectl create -f rbd-pv.yaml</span><br><span class="line">~$ kubectl create -f rbd-pvc.yaml</span><br><span class="line">~$ kubectl get pv</span><br><span class="line">NAME          CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                  STORAGECLASS   REASON   AGE</span><br><span class="line">ceph-rbd-pv   4Gi        RWO            Recycle          Bound    default/ceph-rbd-pvc                           17h</span><br><span class="line">~$ kubectl get pvc</span><br><span class="line">NAME           STATUS   VOLUME        CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">ceph-rbd-pvc   Bound    ceph-rbd-pv   4Gi        RWO                           17h</span><br></pre></td></tr></table></figure>
<h2 id="Ceph-Ansible安装方式"><a href="#Ceph-Ansible安装方式" class="headerlink" title="Ceph-Ansible安装方式"></a><code>Ceph-Ansible</code>安装方式</h2><ul>
<li><a href="https://github.com/ceph/ceph-ansible" target="_blank" rel="noopener">ceph/ceph-ansible</a><h3 id="关于新以太网名命方式"><a href="#关于新以太网名命方式" class="headerlink" title="关于新以太网名命方式"></a>关于新以太网名命方式</h3></li>
<li>新的<code>Linux</code>会以<code>enp0s10</code>这样的方式来替换旧式的<code>ethX</code>命名方式.具体可<a href="https://www.freedesktop.org/wiki/Software/systemd/PredictableNetworkInterfaceNames/" target="_blank" rel="noopener">参照这里 PredictableNetworkInterfaceNames</a>,<a href="https://major.io/2015/08/21/understanding-systemds-predictable-network-device-names/" target="_blank" rel="noopener">Understanding systemd’s predictable network device names</a></li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">enp0s10:</span><br><span class="line">| | |</span><br><span class="line">v | |</span><br><span class="line">en| |   --&gt; ethernet</span><br><span class="line">  v |</span><br><span class="line">  p0|   --&gt; bus number (0)</span><br><span class="line">    v</span><br><span class="line">    s10 --&gt; slot number (10)</span><br></pre></td></tr></table></figure>
<ul>
<li>如果不习惯新式的命名可以通过下面三方法改成旧式的命名方式</li>
<li><p>You basically have three options:</p>
<ol>
<li>You disable the assignment of fixed names, so that the unpredictable kernel names are used again. For this, simply mask udev’s .link file for the default policy: ln -s /dev/null /etc/systemd/network/99-default.link</li>
<li>You create your own manual naming scheme, for example by naming your interfaces “internet0”, “dmz0” or “lan0”. For that create your own .link files in /etc/systemd/network/, that choose an explicit name or a better naming scheme for one, some, or all of your interfaces. See systemd.link(5) for more information.</li>
<li>You pass the net.ifnames=0 on the kernel command line</li>
</ol>
</li>
<li><p>查看虚拟机</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">~$ VBoxManage list vms</span><br><span class="line"><span class="string">"k8s-master"</span> &#123;7bfb1ca4-3ccc-4a1a-8548-7759424df181&#125;</span><br><span class="line"><span class="string">"k8s-node1"</span> &#123;4c29c029-4f93-4463-b83d-4ae9e728e9df&#125;</span><br><span class="line"><span class="string">"k8s-node2"</span> &#123;87a2196c-cf3c-472a-9ffa-f5b8c3e09009&#125;</span><br><span class="line"><span class="string">"k8s-node3"</span> &#123;af9e34cf-a7c9-45d8-ad15-f37d409bcdac&#125;</span><br><span class="line"><span class="string">"k8s-node4"</span> &#123;1f46e865-01c1-4a81-a947-cc267c744756&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 VBoxHeadles启动上述虚拟机,它不会出现窗口.</span></span><br><span class="line">~$ VBoxHeadless --startvm k8s-master</span><br></pre></td></tr></table></figure>
</li>
<li><p>下面是参照官网来安装<code>ceph-deploy</code>.但是使用<code>apt</code>找不到<code>ceph-deploy</code>包名.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">~$ wget -q -O- <span class="string">'https://download.ceph.com/keys/release.asc'</span> | sudo apt-key add -</span><br><span class="line"><span class="comment"># 用Ceph稳定版(如 cuttlefish 、 dumpling 、 emperor 、 firefly,nautilus 等等)替换掉 &#123;ceph-stable-release&#125;</span></span><br><span class="line">~$ <span class="built_in">echo</span> deb http://download.ceph.com/debian-&#123;ceph-stable-release&#125;/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list</span><br><span class="line">~$ sudo apt-get update &amp;&amp; sudo apt-get install ceph-deploy</span><br></pre></td></tr></table></figure>
</li>
<li><p>下面把它转换成<code>Ansible playbook</code>的方式来安装.在<code>Github</code>上的<code>Ceph</code>有一个<a href="https://github.com/ceph/ceph-ansible" target="_blank" rel="noopener">ceph-ansible</a>,没有用过,它的 Star 将近 1k 了.</p>
</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">安装基础软件</span></span><br><span class="line">  <span class="attr">hosts:</span> <span class="string">all</span></span><br><span class="line">  <span class="attr">become:</span> <span class="literal">yes</span></span><br><span class="line">  <span class="comment"># user: root 这里可以直接用root,但是关闭root远程登录后要使用sudo.</span></span><br><span class="line">  <span class="attr">tasks:</span></span><br><span class="line">    <span class="comment"># 参考文档 https://docs.ansible.com/ansible/latest/modules/command_module.html#command-module</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">读取系统发行版本号</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">lsb_release</span> <span class="string">-sc</span></span><br><span class="line">      <span class="attr">register:</span> <span class="string">result</span></span><br><span class="line"></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">安装公钥匙</span></span><br><span class="line">      <span class="attr">apt_key:</span></span><br><span class="line">        <span class="attr">url:</span> <span class="string">https://download.ceph.com/keys/release.asc</span></span><br><span class="line">        <span class="attr">state:</span> <span class="string">present</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 参照文档 https://docs.ansible.com/ansible/latest/modules/apt_repository_module.html?highlight=add%20apt%20repository</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ceph-deploy</span></span><br><span class="line">      <span class="attr">apt_repository:</span></span><br><span class="line">        <span class="attr">repo:</span> <span class="string">deb</span>  <span class="string">http://download.ceph.com/debian-nautilus</span> <span class="string">&#123;&#123;</span> <span class="string">result.stdout</span> <span class="string">&#125;&#125;</span> <span class="string">main</span></span><br><span class="line">        <span class="attr">state:</span> <span class="string">present</span></span><br><span class="line">        <span class="attr">filename:</span> <span class="string">ceph</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 参照文档 https://docs.ansible.com/ansible/latest/modules/apt_key_module.html?highlight=apt%20key</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">添加docker-ce的公钥</span></span><br><span class="line">      <span class="attr">apt_key:</span></span><br><span class="line">        <span class="attr">url:</span> <span class="string">https://download.docker.com/linux/debian/gpg</span></span><br><span class="line">        <span class="attr">state:</span> <span class="string">present</span></span><br><span class="line"></span><br><span class="line">      <span class="comment"># 参照文档 https://docs.ansible.com/ansible/latest/modules/apt_repository_module.html?highlight=add%20apt%20repository</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">docker-ce</span></span><br><span class="line">      <span class="attr">apt_repository:</span></span><br><span class="line">        <span class="attr">repo:</span> <span class="string">deb</span> <span class="string">[arch=amd64]</span> <span class="string">https://download.docker.com/linux/debian</span> <span class="string">&#123;&#123;</span> <span class="string">result.stdout</span> <span class="string">&#125;&#125;</span> <span class="string">stable</span></span><br><span class="line">        <span class="attr">state:</span> <span class="string">present</span></span><br><span class="line">        <span class="attr">filename:</span> <span class="string">docker-ce</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 参照文档 https://docs.ansible.com/ansible/latest/modules/apt_module.html</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">更新并安装</span></span><br><span class="line">      <span class="attr">apt:</span></span><br><span class="line">        <span class="attr">name:</span></span><br><span class="line">          <span class="string">['ntp',</span> <span class="string">'ntpdate'</span><span class="string">,</span> <span class="string">'ntp-doc'</span><span class="string">,</span> <span class="string">'docker-ce'</span><span class="string">,</span> <span class="string">'bridge-utils'</span><span class="string">,</span> <span class="string">'ipvsadm'</span><span class="string">]</span></span><br><span class="line">        <span class="attr">allow_unauthenticated:</span> <span class="literal">yes</span></span><br><span class="line">        <span class="attr">update_cache:</span> <span class="literal">yes</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 参照文档 https://docs.ansible.com/ansible/latest/modules/lineinfile_module.html?highlight=sudoers</span></span><br><span class="line">    <span class="comment"># 如查使用ansible的sysctl模块,可以参照这里 https://docs.ansible.com/ansible/latest/modules/sysctl_module.html?highlight=sysctl</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">更新并安装sysctl</span></span><br><span class="line">      <span class="attr">lineinfile:</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">/etc/sysctl.d/80-k8s.conf</span></span><br><span class="line">        <span class="attr">create:</span> <span class="literal">yes</span></span><br><span class="line">        <span class="attr">line:</span> <span class="string">'<span class="template-variable">&#123;&#123; item &#125;&#125;</span>'</span></span><br><span class="line">      <span class="attr">with_items:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">'net.bridge.bridge-nf-call-ip6tables = 1'</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">'net.bridge.bridge-nf-call-iptables = 1'</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">'net.bridge.bridge-nf-call-arptables = 1'</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">'net.ipv4.ip_forward = 1'</span></span><br><span class="line"></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">更新sysctl</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">sysctl</span> <span class="string">--system</span></span><br><span class="line"></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">block:</span></span><br><span class="line">        <span class="comment"># 命名方式参考这里https://www.freedesktop.org/wiki/Software/systemd/PredictableNetworkInterfaceNames/</span></span><br><span class="line">        <span class="comment"># https://major.io/2015/08/21/understanding-systemds-predictable-network-device-names/</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">使用旧式网卡命名方式</span></span><br><span class="line">          <span class="attr">file:</span></span><br><span class="line">            <span class="attr">src:</span> <span class="string">/dev/null</span></span><br><span class="line">            <span class="attr">dest:</span> <span class="string">/etc/systemd/network/99-default.link</span></span><br><span class="line">            <span class="attr">state:</span> <span class="string">link</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 好像上述改回成旧式命名的方法在debian里不成功.使用下面修改内核参数的方式.</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">更新内核参数</span></span><br><span class="line">          <span class="attr">lineinfile:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/etc/default/grub</span></span><br><span class="line">            <span class="attr">regexp:</span> <span class="string">'^GRUB_CMDLINE_LINUX='</span></span><br><span class="line">            <span class="attr">line:</span> <span class="string">'GRUB_CMDLINE_LINUX="net.ifnames=0 biosdevname=0"'</span></span><br><span class="line"></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">更新grub.cfg</span></span><br><span class="line">          <span class="attr">command:</span> <span class="string">grub-mkconfig</span> <span class="string">-o</span> <span class="string">/boot/grub/grub.cfg</span></span><br></pre></td></tr></table></figure>
<h2 id="安装KubernetesMaster"><a href="#安装KubernetesMaster" class="headerlink" title="安装KubernetesMaster"></a>安装<code>Kubernetes</code>Master</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">~$ sudo kubeadm init --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers  --kubernetes-version v1.14.1 --pod-network-cidr=10.244.0.0/16 --ignore-preflight-errors=NumCPU --apiserver-advertise-address=192.168.99.100</span><br><span class="line">[...]</span><br><span class="line">Your`Kubernetes`control-plane has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user: <span class="comment"># 注意需按照下述步骤进行.先安装对应的网络插件再加入其它的节点</span></span><br><span class="line"></span><br><span class="line">  mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run <span class="string">"kubectl apply -f [podnetwork].yaml"</span> with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">kubeadm join 192.168.99.100:6443 --token ejtj7f.oth6on2k6y0qcj2k \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:d162721230250668a4296aca699867126314a9ecd2418f9c70110b6b02bd01de</span><br><span class="line"></span><br><span class="line"><span class="comment"># 继续安装网络插件.</span></span><br><span class="line">~$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml</span><br></pre></td></tr></table></figure>
<ul>
<li>默认安装集群是使用<code>kube-proxy+iptables</code>模式,需要手动修改为<code>ipvs</code>模式.使用<code>kubectl -n kube-system edit cm kube-proxy</code>打开 ConfigMap 文件,把<code>mode=&quot;&quot;</code>替换成<code>mode=&quot;ipvs</code>,再把旧的 pod 删掉,<code>kubectl -n kube-system delete pod kube-proxy-xxx</code>,它会再生成一个新的 pod.</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">~$ kubectl -n kube-system logs kube-proxy-t27xd</span><br><span class="line">I0514 06:33:30.681150       1 server_others.go:177] Using ipvs Proxier.  ---&gt; 切换成ipvs模式.</span><br><span class="line">W0514 06:33:30.738710       1 proxier.go:381] IPVS scheduler not specified, use rr by default</span><br><span class="line">I0514 06:33:30.747818       1 server.go:555] Version: v1.14.1</span><br><span class="line">[...]</span><br><span class="line"></span><br><span class="line"><span class="comment">#　查看ipvs的列表.</span></span><br><span class="line">~$ sudo ipvsadm -ln</span><br><span class="line">IP Virtual Server version 1.2.1 (size=4096)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span><br><span class="line">TCP  172.17.0.1:32047 rr</span><br><span class="line">TCP  192.168.99.100:32047 rr</span><br><span class="line">TCP  10.0.2.15:32047 rr</span><br><span class="line">TCP  10.96.0.1:443 rr</span><br><span class="line">  -&gt; 192.168.99.100:6443          Masq    1      3          0</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>
<ul>
<li><p>使用<code>Ansible</code>批量加入 k8s 集群.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">~$ cat hosts</span><br><span class="line">[master]</span><br><span class="line">192.168.99.100</span><br><span class="line">[node1]</span><br><span class="line">192.168.99.101</span><br><span class="line">[node2]</span><br><span class="line">192.168.99.102</span><br><span class="line">[node3]</span><br><span class="line">192.168.99.103</span><br><span class="line">[node4]</span><br><span class="line">192.168.99.104</span><br><span class="line">[node]</span><br><span class="line">192.168.99.101</span><br><span class="line">192.168.99.102</span><br><span class="line">192.168.99.103</span><br><span class="line">192.168.99.104</span><br><span class="line"></span><br><span class="line">~<span class="variable">$ansible</span>  -i hosts node -b  -m <span class="built_in">command</span> -a <span class="string">"kubeadm join 192.168.99.100:6443 --token ejtj7f.oth6on2k6y0qcj2k --discovery-token-ca-cert-hash sha256:d162721230250668a4296aca699867126314a9ecd2418f9c70110b6b02bd01de"</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>查看主节点的状态</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">~$ kubectl get nodes</span><br><span class="line">NAME         STATUS     ROLES    AGE   VERSION</span><br><span class="line">k8s-master   NotReady   master   15h   v1.14.1</span><br><span class="line">node1        NotReady   &lt;none&gt;   15h   v1.14.1</span><br><span class="line">node2        NotReady   &lt;none&gt;   15h   v1.14.1</span><br><span class="line">node3        NotReady   &lt;none&gt;   15h   v1.14.1</span><br><span class="line">node4        NotReady   &lt;none&gt;   15h   v1.14.1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看所有节点为什么是NotReady状态?</span></span><br><span class="line">~$ kubectl get pods -n kube-system</span><br><span class="line">NAME                                 READY   STATUS    RESTARTS   AGE</span><br><span class="line">coredns-d5947d4b-kfhlp               0/1     Pending   0          15h</span><br><span class="line">coredns-d5947d4b-sq95j               0/1     Pending   0          15h</span><br><span class="line">etcd-k8s-master                      1/1     Running   2          15h</span><br><span class="line">kube-apiserver-k8s-master            1/1     Running   2          15h</span><br><span class="line">kube-controller-manager-k8s-master   1/1     Running   2          15h</span><br><span class="line">kube-proxy-25vgp                     1/1     Running   2          15h</span><br><span class="line">kube-proxy-75xjc                     1/1     Running   1          15h</span><br><span class="line">kube-proxy-bvdh6                     1/1     Running   1          15h</span><br><span class="line">kube-proxy-lzp8m                     1/1     Running   1          15h</span><br><span class="line">kube-proxy-wnmwk                     1/1     Running   1          15h</span><br><span class="line">kube-scheduler-k8s-master            1/1     Running   2          15h</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看coredns为什么Pending?</span></span><br><span class="line">~$  kubectl describe pod coredns -n kube-system</span><br><span class="line">[...]</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason            Age                  From               Message</span><br><span class="line">  ----     ------            ----                 ----               -------</span><br><span class="line">  Warning  FailedScheduling  10m (x49 over 81m)   default-scheduler  0/5 nodes are available: 5 node(s) had taints that the pod didn\<span class="string">'t tolerate.</span></span><br><span class="line"><span class="string">  Warning  FailedScheduling  75s (x4 over 5m21s)  default-scheduler  0/5 nodes are available: 5 node(s) had taints that the pod didn\'</span>t tolerate.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看系统 journalctl</span></span><br><span class="line">~$ sudo journalctl -u kubelet</span><br><span class="line"><span class="comment"># 发现是因为没有安装网络插件的原因.</span></span><br><span class="line">~$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="使用Rook构建"><a href="#使用Rook构建" class="headerlink" title="使用Rook构建"></a>使用<code>Rook</code>构建</h1><ul>
<li><a href="https://rook.io" target="_blank" rel="noopener">Github Rook</a></li>
<li><a href="https://rook.io/docs/rook/v1.0/" target="_blank" rel="noopener">Rook 文档</a></li>
</ul>
<ul>
<li><code>Ceph Rook</code>集成<br><img src="https://rook.io/docs/rook/v1.0/media/rook-architecture.png" alt="CephRook"></li>
<li>Rook 的架构<br><img src="https://rook.io/docs/rook/v1.0/media/kubernetes.png" alt="rook architecture"></li>
</ul>
<h2 id="安装Ceph"><a href="#安装Ceph" class="headerlink" title="安装Ceph"></a>安装<code>Ceph</code></h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">~$ git <span class="built_in">clone</span> https://github.com/rook/rook</span><br><span class="line">~$ <span class="built_in">cd</span> rook/cluster/examples/kubernetes/ceph/</span><br><span class="line">~$ kubectl create -f common.yaml</span><br><span class="line">~$ kubectl create -f operator.yaml</span><br><span class="line">~$ kubectl create -f cluster.yaml</span><br><span class="line">~$ kubectl -n rook-ceph get pods</span><br><span class="line">NAME                                  READY   STATUS    RESTARTS   AGE</span><br><span class="line">rook-ceph-agent-f7ln5                 1/1     Running   0          5m36s</span><br><span class="line">rook-ceph-agent-fzztf                 1/1     Running   0          5m36s</span><br><span class="line">rook-ceph-agent-mgqk6                 1/1     Running   0          5m36s</span><br><span class="line">rook-ceph-agent-qdbmh                 1/1     Running   0          5m36s</span><br><span class="line">rook-ceph-agent-twsvp                 1/1     Running   0          5m36s</span><br><span class="line">rook-ceph-operator-775cf575c5-8k44f   1/1     Running   1          6m30s</span><br><span class="line">rook-discover-d4btd                   1/1     Running   0          5m36s</span><br><span class="line">rook-discover-fbq9w                   1/1     Running   0          5m36s</span><br><span class="line">rook-discover-gcksv                   1/1     Running   0          5m36s</span><br><span class="line">rook-discover-hnbdj                   1/1     Running   0          5m36s</span><br><span class="line">rook-discover-j5x5h                   1/1     Running   0          5m36s</span><br></pre></td></tr></table></figure>
<h2 id="拆除ROOK"><a href="#拆除ROOK" class="headerlink" title="拆除ROOK"></a>拆除<code>ROOK</code></h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">~$ cat remove-nodes-rooks-containers.sh</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> `seq 1 4`; <span class="keyword">do</span></span><br><span class="line">  <span class="keyword">for</span> n <span class="keyword">in</span> `ansible -i hosts node<span class="variable">$i</span> -b -m <span class="built_in">command</span> -a <span class="string">"docker ps -a"</span> | awk <span class="string">'NR&gt;2 &#123;print $1&#125;'</span>`;<span class="keyword">do</span></span><br><span class="line">    <span class="comment">#ansible -i hosts node$i -b -m command -a "docker stop $n ; docker rm $n";</span></span><br><span class="line">    ansible -i hosts node<span class="variable">$i</span> -b -m <span class="built_in">command</span> -a <span class="string">"docker rm <span class="variable">$n</span>"</span>;</span><br><span class="line">  <span class="keyword">done</span>;</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">~$ cat  remove-rook-cluster-data.sh</span><br><span class="line">ansible -i hosts all -b -m file -a <span class="string">"path=/var/lib/rook  state=absent"</span></span><br><span class="line">ansible -i hosts all -b -m file -a <span class="string">"path=/etc/kubernetes  state=absent"</span></span><br><span class="line">ansible -i hosts all -b -m file -a <span class="string">"path=/var/lib/kubelet state=absent"</span></span><br></pre></td></tr></table></figure>
<h2 id="错误"><a href="#错误" class="headerlink" title="错误"></a>错误</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">~$ kubectl -n rook-ceph get pod</span><br><span class="line">NAME                               READY   STATUS                  RESTARTS   AGE</span><br><span class="line">rook-ceph-mon<span class="_">-a</span>-f799d9cf6-xrg8f    0/1     Init:CrashLoopBackOff   6          8m46s</span><br><span class="line">rook-ceph-mon<span class="_">-d</span>-5dd7b4d56f-wwg8n   0/1     Init:CrashLoopBackOff   6          7m1s</span><br><span class="line">rook-ceph-mon<span class="_">-f</span>-7977bd98c9-9b6h4   0/1     Init:CrashLoopBackOff   5          5m19s</span><br><span class="line"></span><br><span class="line">~$ kubectl -n rook-ceph  describe pod rook-ceph-mon<span class="_">-a</span></span><br><span class="line">[...]</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason     Age                     From                 Message</span><br><span class="line">  ----     ------     ----                    ----                 -------</span><br><span class="line">  Normal   Scheduled  9m15s                   default-scheduler    Successfully assigned rook-ceph/rook-ceph-mon<span class="_">-a</span>-f799d9cf6-xrg8f to k8s-master</span><br><span class="line">  Normal   Pulled     7m20s (x5 over 9m4s)    kubelet, k8s-master  Container image <span class="string">"rook/ceph:v0.9.3"</span> already present on machine</span><br><span class="line">  Normal   Created    7m19s (x5 over 9m2s)    kubelet, k8s-master  Created container config-init</span><br><span class="line">  Normal   Started    7m18s (x5 over 8m59s)   kubelet, k8s-master  Started container config-init</span><br><span class="line">  Warning  BackOff    3m52s (x26 over 8m52s)  kubelet, k8s-master  Back-off restarting failed container</span><br><span class="line"></span><br><span class="line">~$ kubectl -n rook-ceph  describe pod rook-ceph-mon<span class="_">-d</span></span><br><span class="line">[...]</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason     Age                     From               Message</span><br><span class="line">  ----     ------     ----                    ----               -------</span><br><span class="line">  Normal   Scheduled  8m2s                    default-scheduler  Successfully assigned rook-ceph/rook-ceph-mon<span class="_">-d</span>-5dd7b4d56f-wwg8n to node1</span><br><span class="line">  Normal   Pulled     6m15s (x5 over 7m45s)   kubelet, node1     Container image <span class="string">"rook/ceph:v0.9.3"</span> already present on machine</span><br><span class="line">  Normal   Created    6m15s (x5 over 7m45s)   kubelet, node1     Created container config-init</span><br><span class="line">  Normal   Started    6m14s (x5 over 7m45s)   kubelet, node1     Started container config-init</span><br><span class="line">  Warning  BackOff    2m41s (x26 over 7m43s)  kubelet, node1     Back-off restarting failed container</span><br></pre></td></tr></table></figure>
<h1 id="谢谢支持"><a href="#谢谢支持" class="headerlink" title="谢谢支持"></a>谢谢支持</h1><ul>
<li>微信二维码:</li>
</ul>
<p><img src="/imgs/mm_reward_qrcode_1525013906055.png" width="40%" height="40%" align="center/"></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Kubernetes-Ceph-Docker/" rel="tag"># Kubernetes,Ceph,Docker</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/02/13/Kubernetes使用指南/" rel="prev" title="Kubernetes使用指南">
      <i class="fa fa-chevron-left"></i> Kubernetes使用指南
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/02/27/搭建一些开源的工具服务/" rel="next" title="搭建一些开源的工具服务">
      搭建一些开源的工具服务 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#数据卷-Volumes"><span class="nav-number">1.</span> <span class="nav-text">数据卷(Volumes)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#本地数据卷"><span class="nav-number">1.1.</span> <span class="nav-text">本地数据卷</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#HostPath"><span class="nav-number">1.1.1.</span> <span class="nav-text">HostPath</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Ceph集群"><span class="nav-number">2.</span> <span class="nav-text">Ceph集群</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#概要"><span class="nav-number">2.1.</span> <span class="nav-text">概要</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#数据卷"><span class="nav-number">2.1.1.</span> <span class="nav-text">数据卷</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ceph的功能组件"><span class="nav-number">2.2.</span> <span class="nav-text">Ceph的功能组件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ceph-功能特性"><span class="nav-number">2.3.</span> <span class="nav-text">Ceph 功能特性</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RADOS"><span class="nav-number">2.3.1.</span> <span class="nav-text">RADOS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ceph文件系统"><span class="nav-number">2.3.2.</span> <span class="nav-text">Ceph文件系统</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ceph块设备"><span class="nav-number">2.3.3.</span> <span class="nav-text">Ceph块设备</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ceph对象网关"><span class="nav-number">2.3.4.</span> <span class="nav-text">Ceph对象网关</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#通过Ceph-ceph-ansiable安装"><span class="nav-number">2.4.</span> <span class="nav-text">通过Ceph/ceph-ansiable安装</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#通过Ceph-ceph-deploy安装"><span class="nav-number">2.5.</span> <span class="nav-text">通过Ceph/ceph-deploy安装</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#快速安装-apt"><span class="nav-number">2.5.1.</span> <span class="nav-text">快速安装(apt)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#清理旧节点"><span class="nav-number">2.5.2.</span> <span class="nav-text">清理旧节点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#安装节点"><span class="nav-number">2.5.3.</span> <span class="nav-text">安装节点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ceph-Manager部署"><span class="nav-number">2.5.4.</span> <span class="nav-text">Ceph Manager部署</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ceph-OSD部署"><span class="nav-number">2.5.5.</span> <span class="nav-text">Ceph OSD部署</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Parted-GPT-分区"><span class="nav-number">2.5.6.</span> <span class="nav-text">Parted(GPT 分区)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创建MDS服务器"><span class="nav-number">2.5.7.</span> <span class="nav-text">创建MDS服务器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#civetweb配置"><span class="nav-number">2.5.8.</span> <span class="nav-text">civetweb配置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#客户端访问"><span class="nav-number">2.6.</span> <span class="nav-text">客户端访问</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#创建S3用户"><span class="nav-number">2.6.1.</span> <span class="nav-text">创建S3用户</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创建Swift用户"><span class="nav-number">2.6.2.</span> <span class="nav-text">创建Swift用户</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用Python客户端库测试"><span class="nav-number">2.6.3.</span> <span class="nav-text">使用Python客户端库测试</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用s3cmd测试"><span class="nav-number">2.6.4.</span> <span class="nav-text">使用s3cmd测试</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#s3fs-fuse挂载文件系统"><span class="nav-number">2.6.5.</span> <span class="nav-text">s3fs-fuse挂载文件系统</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#警告错误类"><span class="nav-number">2.7.</span> <span class="nav-text">警告错误类</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#与Kubernetes集成"><span class="nav-number">3.</span> <span class="nav-text">与Kubernetes集成</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#创建RBD"><span class="nav-number">3.1.</span> <span class="nav-text">创建RBD</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ceph-Ansible安装方式"><span class="nav-number">3.2.</span> <span class="nav-text">Ceph-Ansible安装方式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#关于新以太网名命方式"><span class="nav-number">3.2.1.</span> <span class="nav-text">关于新以太网名命方式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#安装KubernetesMaster"><span class="nav-number">3.3.</span> <span class="nav-text">安装KubernetesMaster</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#使用Rook构建"><span class="nav-number">4.</span> <span class="nav-text">使用Rook构建</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#安装Ceph"><span class="nav-number">4.1.</span> <span class="nav-text">安装Ceph</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#拆除ROOK"><span class="nav-number">4.2.</span> <span class="nav-text">拆除ROOK</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#错误"><span class="nav-number">4.3.</span> <span class="nav-text">错误</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#谢谢支持"><span class="nav-number">5.</span> <span class="nav-text">谢谢支持</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="yjdwbj"
      src="https://avatars0.githubusercontent.com/u/321919?s=400&u=67298bd9fc3b622eaf93a6cc511a24ac878e86db&v=4">
  <p class="site-author-name" itemprop="name">yjdwbj</p>
  <div class="site-description" itemprop="description">最是人间留不住,朱颜辞镜花辞树</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">50</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">43</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yjdwbj</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/muse.js"></script>
<script src="/js/next-boot.js"></script>



  















  

  

</body>
</html>
